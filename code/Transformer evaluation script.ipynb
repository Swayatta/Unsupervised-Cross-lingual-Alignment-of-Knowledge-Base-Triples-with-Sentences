{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Transformer evaluation script.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"solar-upset"},"source":["import os\n","import json\n","import torch.nn.functional as F\n","import torch"],"id":"solar-upset","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owned-polls"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"id":"owned-polls","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"appointed-semiconductor","outputId":"8b5316e4-5709-48d0-b3d0-0303852bc8de"},"source":["print('using device:', device)"],"id":"appointed-semiconductor","execution_count":null,"outputs":[{"output_type":"stream","text":["using device: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"incredible-currency"},"source":["root_dir='/home/tushar.abhishek/ire/research/project_copernicus/sample_annotations/hi'\n","\n","sentences_file = os.path.join(root_dir, 'query', 'hindi_sample_random_sentences.json')\n","response_file = os.path.join(root_dir, 'response', 'annotations_406_v1')"],"id":"incredible-currency","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"further-occasion"},"source":["with open(sentences_file) as dfile:\n","    sent_data = json.load(dfile)"],"id":"further-occasion","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"geographic-technique"},"source":["with open(response_file) as dfile:\n","    ann_data = json.load(dfile)"],"id":"geographic-technique","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"photographic-opening","outputId":"8fc7bff3-c729-48c0-e762-627c47870a65"},"source":["print('total count of sentences %d' % len(sent_data))\n","print('total response recorded %d' % len(ann_data))"],"id":"photographic-opening","execution_count":null,"outputs":[{"output_type":"stream","text":["total count of sentences 27246\n","total response recorded 406\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lucky-invention","outputId":"c1837c5e-e9eb-4930-910a-917226596860"},"source":["for item in ann_data:\n","    print(item)\n","    break"],"id":"lucky-invention","execution_count":null,"outputs":[{"output_type":"stream","text":["{'_id': {'$oid': '60ac87e2230a5f0015dc74ee'}, 'facts': [['occupation', 'cricketer', []], ['member of sports team', 'India national cricket team', [['start time', '1981'], ['end time', '1992']]], ['country for sport', 'India', []]], 'factIndex': [4, 10, 14], 'email': 'shivprasad.sagare@research.iiit.ac.in', 'sentence': 'इन्होंने बतौर क्रिकेट खिलाड़ी भारतीय क्रिकेट टीम के लिए १९८१ से १९९२ तक टेस्ट क्रिकेट और एक दिवसीय अंतर्राष्ट्रीय क्रिकेट खेला है।', 'sentenceOffset': 0, 'qid': 'Q3518502', 'covers': 'Y', 'date': {'$date': '2021-05-25T05:15:14.852Z'}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cognitive-tutorial"},"source":["qid_sentence_data = {}\n","\n","\n","for item in sent_data:\n","    qid = item['qid']\n","    if qid not in qid_sentence_data:\n","        qid_sentence_data[qid]={\n","            'facts': item['facts'],\n","            'sentences': [[] for x in range(item['sentence-count'])],\n","        }\n","    sentence_index = item['sentence-offset']\n","    qid_sentence_data[qid]['sentences'][sentence_index]=item['sentence']"],"id":"cognitive-tutorial","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"limiting-storm","outputId":"aa5b6528-0818-4c77-f866-93b1ce7551e0"},"source":["print('total number of entities found', len(qid_sentence_data))\n","\n","from collections import defaultdict\n","empty_sentence = defaultdict(lambda: [])\n","#checking if any empty sentence exists\n","\n","empty_count=0\n","for qid in qid_sentence_data:\n","    for i, item in enumerate(qid_sentence_data[qid]['sentences']):\n","        if len(item)==0:\n","            empty_sentence[qid].append(i)\n","            empty_count+=1\n","\n","print('total number of empty sentences', empty_count)"],"id":"limiting-storm","execution_count":null,"outputs":[{"output_type":"stream","text":["total number of entities found 9922\n","total number of empty sentences 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"connected-mills","outputId":"5241e8e9-4e41-43d2-ad0f-7a49440c93a2"},"source":["import random\n","random_key = random.choice(list(qid_sentence_data.keys()))\n","\n","print(random_key)\n","\n","print(qid_sentence_data[random_key])"],"id":"connected-mills","execution_count":null,"outputs":[{"output_type":"stream","text":["Q62604398\n","{'facts': [['date of birth', '21 January 1967', []], ['instance of', 'human', []], ['sex or gender', 'male', []], ['position held', 'Member of the 17th Lok Sabha', [['parliamentary group', 'Bharatiya Janata Party'], ['electoral district', 'Karnal Lok Sabha constituency'], ['parliamentary term', '17th Lok Sabha']]], ['member of political party', 'Bharatiya Janata Party', []], ['country of citizenship', 'India', []], ['occupation', 'politician', []], ['educated at', 'Kurukshetra University', []], ['place of birth', 'Panipat', []]], 'sentences': ['संजय भाटिया हरियाणा से संसद के निर्वाचित सदस्य हैं।']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"violent-visiting"},"source":["from transformers import AutoTokenizer, AutoModel\n","\n","# config=\"xlm-roberta-large\" \n","config=\"google/muril-base-cased\"\n","# config=\"ai4bharat/indic-bert\"\n","# config=\"sentence-transformers/LaBSE\"\n","# config=\"bert-base-multilingual-uncased\"\n","# config=\"facebook/mbart-large-cc25\" # also need to change\n","\n","tokenizer = AutoTokenizer.from_pretrained(config)\n","model = AutoModel.from_pretrained(config).to(device)"],"id":"violent-visiting","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"structured-burden"},"source":["random_data = random.choice([x for x in ann_data if len(x['factIndex'])])\n","random_qid = random_data['qid']\n","\n","fact_list = [\"%s is %s\"%(x[0], x[1]) for x in qid_sentence_data[random_qid]['facts']]\n","sentence_list = qid_sentence_data[random_qid]['sentences']\n","\n","target_sentence = sentence_list[random_data['sentenceOffset']]\n","fact_indexes = random_data['factIndex']"],"id":"structured-burden","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shaped-motel"},"source":["import torch\n","import torch.nn as nn\n","model.eval()\n","\n","def pooled_rep(model_output, attention_mask, reduce='cls'):\n","    if reduce=='cls':\n","        return model_output[:, 0, :]\n","    elif reduce == \"mean\":\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","    elif reduce == 'sum':\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        return sum_embeddings\n","    else:\n","        raise Exception('reduce function not present !!!')\n","\n","with torch.no_grad():\n","    enc = tokenizer.batch_encode_plus(sentence_list, padding='longest', return_attention_mask=True, return_tensors='pt')\n","    #taking the [CLS] token\n","    s_out = model(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device))[0]\n","    sentence_encoding = pooled_rep(s_out, enc[\"attention_mask\"].to(device), reduce='mean')\n","    \n","    fenc = tokenizer.batch_encode_plus(fact_list, padding='longest', return_attention_mask=True, return_tensors='pt')\n","    f_out = model(input_ids=fenc[\"input_ids\"].to(device), attention_mask=fenc[\"attention_mask\"].to(device))[0]\n","    facts_encoding = pooled_rep(f_out, fenc[\"attention_mask\"].to(device), reduce='mean')"],"id":"shaped-motel","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"australian-spank","outputId":"16d57be4-52ae-4eff-9e69-d2ee39465f15"},"source":["# simple cosine similarity score\n","\n","print(\"qid\", random_qid)\n","print(target_sentence, end='\\n\\n')\n","with torch.no_grad():\n","    scores = F.cosine_similarity(facts_encoding, sentence_encoding[random_data['sentenceOffset']].unsqueeze(0), 1, 1e-6).cpu().tolist()\n","    score_map = {i:v for i,v in enumerate(scores)}\n","    for j, u in sorted(score_map.items(), key=lambda x: x[1], reverse=True):\n","        temp_string = \"%0.3f %s\" % (u, fact_list[j])\n","        if j in fact_indexes:\n","            print(\"* %s\" % temp_string)\n","        else:\n","            print(temp_string)"],"id":"australian-spank","execution_count":null,"outputs":[{"output_type":"stream","text":["qid Q317969\n","अब्दुस्सत्तार बांग्लादेश के राष्ट्रपति थे।\n","\n","* 0.992 position held is President of Bangladesh\n","0.989 member of political party is Bangladesh Nationalist Party\n","0.989 occupation is politician\n","0.989 religion is Islam\n","0.989 country of citizenship is Bangladesh\n","0.989 place of birth is Birbhum district\n","0.989 place of death is Dhaka\n","0.988 country of citizenship is British Raj\n","0.988 instance of is human\n","0.988 country of citizenship is Pakistan\n","0.988 native language is Bengali\n","0.988 educated at is University of Calcutta\n","0.988 date of birth is 1906\n","0.987 date of death is 05 October 1985\n","0.986 sex or gender is male\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"altered-batch","outputId":"d082d469-aab9-4323-f2b4-9820cbf1f3df"},"source":["# cosine similarity score using neighbour information\n","# refer to Unsupervised parallel data mining section of \n","# this paper: https://arxiv.org/pdf/2006.09526.pdf for more details\n","\n","import numpy as np\n","neighbour=5\n","\n","\n","print(\"qid\", random_qid)\n","print(target_sentence, end='\\n\\n')\n","with torch.no_grad():\n","    g_scores = F.cosine_similarity(facts_encoding, sentence_encoding[random_data['sentenceOffset']].unsqueeze(0), 1, 1e-6).cpu().tolist()\n","    score_map = {}\n","    sentence_neighbours_enc = F.cosine_similarity(sentence_encoding, sentence_encoding[random_data['sentenceOffset']].unsqueeze(0))\n","    sentence_scores = [x.item() for i, x in enumerate(sentence_neighbours_enc) if i!=random_data['sentenceOffset']]\n","    sent_k_score = max(np.sum(sentence_scores[:neighbour]), 0)\n","    sent_k = max(len(sentence_scores[:neighbour]), 1)\n","    for i, _ in enumerate(fact_list):\n","        temp_fact_cosine = F.cosine_similarity(facts_encoding, facts_encoding[i].unsqueeze(0))\n","        facts_scores = [x.item() for j, x in enumerate(temp_fact_cosine) if i!=j]\n","        fact_k_score = max(np.sum(facts_scores[:neighbour]), 0)\n","        fact_k = max(len(facts_scores[:neighbour]), 1)\n","        denom = (fact_k_score/(2*fact_k)) + (sent_k_score/(2*sent_k))\n","        score_map[i] = g_scores[i]/denom\n","    \n","    for j, u in sorted(score_map.items(), key=lambda x: x[1], reverse=True):\n","        temp_string = \"%0.3f %s\" % (u, fact_list[j])\n","        if j in fact_indexes:\n","            print(\"* %s\" % temp_string)\n","        else:\n","            print(temp_string)"],"id":"altered-batch","execution_count":null,"outputs":[{"output_type":"stream","text":["qid Q317969\n","अब्दुस्सत्तार बांग्लादेश के राष्ट्रपति थे।\n","\n","* 1.000 position held is President of Bangladesh\n","0.998 member of political party is Bangladesh Nationalist Party\n","0.997 educated at is University of Calcutta\n","0.997 occupation is politician\n","0.997 religion is Islam\n","0.996 country of citizenship is Bangladesh\n","0.996 place of birth is Birbhum district\n","0.996 instance of is human\n","0.996 native language is Bengali\n","0.996 place of death is Dhaka\n","0.996 country of citizenship is British Raj\n","0.995 country of citizenship is Pakistan\n","0.995 sex or gender is male\n","0.995 date of death is 05 October 1985\n","0.995 date of birth is 1906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"proper-wound"},"source":["def get_score(true_y, pred_y):\n","    # calculates the precision and recall on set\n","    set_a = set(true_y)\n","    set_b = set(pred_y)\n","    if len(set_a)==0 or len(set_b)==0:\n","        return 0.0, 0.0\n","    # precision, recall, f1\n","    tp = set_a.intersection(set_b)\n","    precision = len(tp)/float(len(pred_y))\n","    recall = len(tp)/float(len(true_y))\n","    return precision, recall"],"id":"proper-wound","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"concerned-registrar"},"source":["import numpy as np\n","\n","def fact_str(fact, enable_qualifiers=False):\n","    fact_str = fact[0:2]\n","    qualifier_str = [' '.join(x) for x in fact[2]]\n","    if enable_qualifiers:\n","        fact_str.extend(qualifier_str)\n","    return fact_str\n","\n","def pooled_rep(model_output, attention_mask, reduce='cls'):\n","    if reduce=='cls':\n","        return model_output[:, 0, :]\n","    elif reduce == \"mean\":\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","    elif reduce == 'sum':\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        return sum_embeddings\n","    else:\n","        raise Exception('reduce function not present !!!')\n","\n","def get_sentence_specific_fact_alignment(tsentences, tfacts, index, threshold=0.5, score_type='cosine', reduce='cls'):\n","    res = []\n","    with torch.no_grad():\n","        enc = tokenizer.batch_encode_plus(tsentences, padding='longest', return_attention_mask=True, return_tensors='pt')\n","        #taking the [CLS] token\n","        s_out = model(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device))[0]\n","        sentence_encoding = pooled_rep(s_out, enc[\"attention_mask\"].to(device), reduce=reduce)\n","        \n","        processed_facts = [' is '.join(fact_str(x)) for x in tfacts]\n","        fenc = tokenizer.batch_encode_plus(processed_facts, padding='longest', return_attention_mask=True, return_tensors='pt')\n","        f_out = model(input_ids=fenc[\"input_ids\"].to(device), attention_mask=fenc[\"attention_mask\"].to(device))[0]\n","        facts_encoding = pooled_rep(f_out, fenc[\"attention_mask\"].to(device), reduce=reduce)\n","        \n","        scores = F.cosine_similarity(facts_encoding, sentence_encoding[index].unsqueeze(0), 1, 1e-6).cpu().tolist()\n","        if score_type=='cosine':\n","            score_map = {i:v for i,v in enumerate(scores)}\n","        else:\n","            neighbour = 5\n","            sentence_neighbours_enc = F.cosine_similarity(sentence_encoding, sentence_encoding[index].unsqueeze(0))\n","            sentence_scores = [x.item() for i, x in enumerate(sentence_neighbours_enc) if i!=index]\n","            sent_k_score = max(np.sum(sentence_scores[:neighbour]), 0)\n","            sent_k = max(len(sentence_scores[:neighbour]), 1)\n","            \n","            score_map = {}\n","            for i, _ in enumerate(tfacts):\n","                temp_fact_cosine = F.cosine_similarity(facts_encoding, facts_encoding[i].unsqueeze(0))\n","                facts_scores = [x.item() for j, x in enumerate(temp_fact_cosine) if i!=j]\n","                fact_k_score = max(np.sum(facts_scores[:neighbour]), 0)\n","                fact_k = max(len(facts_scores[:neighbour]), 1)\n","                denom = (fact_k_score/(2*fact_k)) + (sent_k_score/(2*sent_k))\n","                score_map[i] = scores[i]/denom\n","        \n","        max_facts = int(threshold*len(score_map))\n","        for j, u in sorted(score_map.items(), key=lambda x: x[1], reverse=True):\n","            if max_facts<1:\n","                break\n","            res.append(j)\n","            max_facts-=1\n","        return res"],"id":"concerned-registrar","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"closing-convert","outputId":"ae623916-597b-45c3-a3c2-913d85675d78"},"source":["# evluating on the annotated dataset\n","\n","from tqdm import tqdm\n","import numpy as np\n","from collections import defaultdict\n","\n","partial_coverage_scores = []\n","full_coverage_scores = []\n","combined_scores = []\n","\n","results = defaultdict(lambda: defaultdict())\n","\n","model.eval()\n","for item in tqdm(ann_data):\n","    qid = item['qid']\n","    sent_index = item['sentenceOffset']\n","    true_fact_index = item['factIndex']\n","    # don't consider the empty facts datainstances\n","    if not len(true_fact_index):\n","        continue\n","    t_qid_data = qid_sentence_data[qid]\n","    \n","    pred_fact_index = get_sentence_specific_fact_alignment(t_qid_data['sentences'], \n","                                                            t_qid_data['facts'], \n","                                                               sent_index, threshold=0.5,\n","                                                          score_type='cosine', reduce='mean')\n","\n","\n","    results[qid][sent_index] = {'annotated': true_fact_index, 'predicted': pred_fact_index}\n","    score = get_score(true_fact_index, pred_fact_index)\n","    if item['covers']=='Y':\n","        full_coverage_scores.append(score)\n","    else:\n","        partial_coverage_scores.append(score)\n","    combined_scores.append(score)\n","\n","print('combined [%d] | avg_precision : %f, avg_recall : %f' % (len(combined_scores), np.mean([x[0] for x in combined_scores]), np.mean([x[1] for x in combined_scores])))\n","if len(full_coverage_scores):\n","    print('full coverage [%d] | avg_precision : %f, avg_recall : %f' % (len(full_coverage_scores), np.mean([x[0] for x in full_coverage_scores]), np.mean([x[1] for x in full_coverage_scores])))\n","if len(partial_coverage_scores):\n","    print('partial coverage [%d] | avg_precision : %f, avg_recall : %f' % (len(partial_coverage_scores), np.mean([x[0] for x in partial_coverage_scores]), np.mean([x[1] for x in partial_coverage_scores])))"],"id":"closing-convert","execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 406/406 [07:19<00:00,  1.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["combined [250] | avg_precision : 0.299082, avg_recall : 0.785467\n","full coverage [111] | avg_precision : 0.288798, avg_recall : 0.846085\n","partial coverage [139] | avg_precision : 0.307294, avg_recall : 0.737059\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eastern-tulsa","outputId":"6e742717-3582-4dc3-8609-27ee82534f50"},"source":["import random\n","\n","random_annotation = random.choice([x for x in ann_data if x['covers']=='Y'])\n","random_qid = random_annotation['qid']\n","random_sent_idx = random_annotation['sentenceOffset']\n","\n","print('QID', random_qid)\n","print('annotator', random_annotation['email'])\n","print('--'*30)\n","print('prompt sentence [%d] : %s' % (random_sent_idx, qid_sentence_data[random_qid]['sentences'][random_sent_idx]))\n","print('actual sentence : %s' % random_annotation['sentence'])\n","print('--'*30)\n","print('actual index', results[random_qid][random_sent_idx]['annotated'])\n","print('predicted index', results[random_qid][random_sent_idx]['predicted'])\n","print('--'*30)\n","print('all candidate facts')\n","for fi, _fact in enumerate(qid_sentence_data[random_qid]['facts']):\n","    print(\"%d> %s\" % (fi, _fact))\n","print('=='*30)\n","random_sample_score = get_score(results[random_qid][random_sent_idx]['annotated'],\n","                               results[random_qid][random_sent_idx]['predicted'])\n","print('precision: %0.2f, recall: %0.2f' % (random_sample_score[0], random_sample_score[1]))\n","print('=='*30)\n","print('annotated facts > ')\n","for fi in results[random_qid][random_sent_idx]['annotated']:\n","    print(\"%d> %s\" % (fi, qid_sentence_data[random_qid]['facts'][fi]))\n","print('~~'*30)\n","print('predicted facts > ')\n","for fi in results[random_qid][random_sent_idx]['predicted']:\n","    print(\"%d> %s\" % (fi, qid_sentence_data[random_qid]['facts'][fi]))"],"id":"eastern-tulsa","execution_count":null,"outputs":[{"output_type":"stream","text":["QID Q6893010\n","annotator tushar.abhishek@ymail.com\n","------------------------------------------------------------\n","prompt sentence [2] : 27 नवम्बर 2018 को उनका मुम्बई में निधन हो गया।\n","actual sentence : 27 नवम्बर 2018 को उनका मुम्बई में निधन हो गया।\n","------------------------------------------------------------\n","actual index [1, 11]\n","predicted index [1, 11, 3, 0, 2, 4, 9, 12, 7]\n","------------------------------------------------------------\n","all candidate facts\n","0> ['date of birth', '02 July 1954', []]\n","1> ['date of death', '27 November 2018', []]\n","2> ['work period (start)', '1982', []]\n","3> ['work period (end)', '2018', []]\n","4> ['instance of', 'human', []]\n","5> ['sex or gender', 'male', []]\n","6> ['religion', 'Islam', []]\n","7> ['country of citizenship', 'India', []]\n","8> ['given name', 'Mohammed', []]\n","9> ['place of birth', 'Kolkata', []]\n","10> ['occupation', 'playback singer', []]\n","11> ['place of death', 'Mumbai', []]\n","12> ['family name', 'Aziz', []]\n","13> ['languages spoken, written or signed', 'Bengali', []]\n","14> ['languages spoken, written or signed', 'Hindi', []]\n","15> ['nominated for', 'Filmfare Award for Best Male Playback Singer', [['point in time', '1990'], ['for work', 'Ram Lakhan']]]\n","16> ['nominated for', 'Filmfare Award for Best Male Playback Singer', [['point in time', '1989'], ['for work', 'Dayavan']]]\n","17> ['award received', 'Filmfare Award for Best Male Playback Singer', [['point in time', '1989'], ['for work', 'Dayavan']]]\n","============================================================\n","precision: 0.22, recall: 1.00\n","============================================================\n","annotated facts > \n","1> ['date of death', '27 November 2018', []]\n","11> ['place of death', 'Mumbai', []]\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","predicted facts > \n","1> ['date of death', '27 November 2018', []]\n","11> ['place of death', 'Mumbai', []]\n","3> ['work period (end)', '2018', []]\n","0> ['date of birth', '02 July 1954', []]\n","2> ['work period (start)', '1982', []]\n","4> ['instance of', 'human', []]\n","9> ['place of birth', 'Kolkata', []]\n","12> ['family name', 'Aziz', []]\n","7> ['country of citizenship', 'India', []]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"correct-choice"},"source":[""],"id":"correct-choice","execution_count":null,"outputs":[]}]}