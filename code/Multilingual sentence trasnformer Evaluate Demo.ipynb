{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multilingual sentence trasnformer Evaluate Demo.ipynb","provenance":[{"file_id":"1YySiXuOmMivpKkU4Wv9Y0atNXzWhjVWW","timestamp":1620066425124}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e6680ad81b564e1a883b5974545d514a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b391eec0938e41aa89cbfce3f2e833dc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_096c474af4bf498d8bf093780882d21b","IPY_MODEL_e164837e51b24e24bd48d1dbc18232e3"]}},"b391eec0938e41aa89cbfce3f2e833dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"096c474af4bf498d8bf093780882d21b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_235accd8c2ee4e508d06de9ff66d0f50","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1014059776,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1014059776,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af2a962b7933472f9fd169a270526a10"}},"e164837e51b24e24bd48d1dbc18232e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_824ad8e9c78e46cb89ed0d1ceaf4f61a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.01G/1.01G [00:36&lt;00:00, 27.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82dc08e4adc74f06bf327c46d680ae88"}},"235accd8c2ee4e508d06de9ff66d0f50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af2a962b7933472f9fd169a270526a10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"824ad8e9c78e46cb89ed0d1ceaf4f61a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82dc08e4adc74f06bf327c46d680ae88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"laesoYf5GYPj"},"source":["# Transformer based models"]},{"cell_type":"markdown","metadata":{"id":"aTJQH4nkGYPz"},"source":["### Importing the required libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lp4BpCrGYP1","executionInfo":{"status":"ok","timestamp":1625878465240,"user_tz":-330,"elapsed":2140,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"13e25b7f-c581-4964-f952-f886b82ab1e5"},"source":["import io\n","import numpy as np\n","# from google_trans_new import google_translator \n","from scipy.spatial.distance import cosine\n","import json \n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","from nltk import word_tokenize\n","from itertools import groupby \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lbppqNkwWjZQ"},"source":["from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6pRWUbPG_1n","executionInfo":{"status":"ok","timestamp":1625883415261,"user_tz":-330,"elapsed":22286,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"a08bf74f-46c4-458e-850f-32077650e035"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CJ0vkAXXPdLo"},"source":["!pip install -U torch   \n","import torch\n","!pip install -U transformers\n","# !pip install -U sentence-transformers\n","import transformers\n","# from sentence_transformers import SentenceTransformer,util\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWy7duWH7Rg-","executionInfo":{"status":"ok","timestamp":1625883285182,"user_tz":-330,"elapsed":716,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"fdedaa22-87cb-4419-e5ee-ef0b8dc22224"},"source":["import os\n","import json\n","import torch.nn.functional as F\n","import torch\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tcp0iJ0OfJAR","executionInfo":{"status":"ok","timestamp":1625888178739,"user_tz":-330,"elapsed":3617,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"5971e453-acc8-4f62-9c3c-f4197cb49bf8"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 32.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 18.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 9.1MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7zQOoUxPpgS","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["e6680ad81b564e1a883b5974545d514a","b391eec0938e41aa89cbfce3f2e833dc","096c474af4bf498d8bf093780882d21b","e164837e51b24e24bd48d1dbc18232e3","235accd8c2ee4e508d06de9ff66d0f50","af2a962b7933472f9fd169a270526a10","824ad8e9c78e46cb89ed0d1ceaf4f61a","82dc08e4adc74f06bf327c46d680ae88"]},"executionInfo":{"status":"ok","timestamp":1620066612832,"user_tz":-330,"elapsed":137767,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"013e1b04-ecee-428c-df89-79af494fea10"},"source":["          #### Sentence Transformer models ####\n","# model = SentenceTransformer('stsb-xlm-r-multilingual')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","# model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6680ad81b564e1a883b5974545d514a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1014059776.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"_BN79Tzr2AVF","executionInfo":{"status":"error","timestamp":1625888359456,"user_tz":-330,"elapsed":3739,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"75b107b4-1fcf-4551-e508-70bdad98cbfa"},"source":["          #### Vanilla transformer models\n","import transformers\n","import sentencepiece\n","from transformers import AutoTokenizer, AutoModel\n","\n","# config=\"xlm-roberta-large\" \n","# config=\"google/muril-base-cased\"\n","config=\"ai4bharat/indic-bert\"\n","# config=\"sentence-transformers/LaBSE\"\n","# config=\"bert-base-multilingual-uncased\"\n","# config=\"facebook/mbart-large-cc25\" # also need to change\n","\n","tokenizer = AutoTokenizer.from_pretrained(config)\n","model = AutoModel.from_pretrained(config).to(device)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-254ea895af2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# config=\"facebook/mbart-large-cc25\" # also need to change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         return cls._from_pretrained(\n\u001b[0;32m-> 1720\u001b[0;31m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1721\u001b[0m         )\n\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/albert/tokenization_albert_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mcls_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             raise ValueError(\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m\"(2) a slow tokenizer instance to convert or \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."]}]},{"cell_type":"code","metadata":{"id":"e8JDa8oF1xrb"},"source":["### Using pooled output if required\n","import torch\n","import torch.nn as nn\n","model.eval()\n","\n","def fact_str(fact, enable_qualifiers=False):\n","    fact_str = fact[1:3]\n","    qualifier_str = [' '.join(x) for x in fact[2]]\n","    if enable_qualifiers:\n","        fact_str.extend(qualifier_str)\n","    return fact_str\n","\n","def pooled_rep(model_output, attention_mask, reduce='cls'):\n","    if reduce=='cls':\n","        return model_output[:, 0, :]\n","    elif reduce == \"mean\":\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","    elif reduce == 'sum':\n","        token_embeddings = model_output #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        return sum_embeddings\n","    else:\n","        raise Exception('reduce function not present !!!')\n","\n","fact_list = [\"I robbed bank\",\"I like dogs\"]\n","sentence_list = [\"मुझे कुत्ते अच्छे लगते हैं\"]\n","# fact_list = []\n","with torch.no_grad():\n","    enc = tokenizer.batch_encode_plus(sentence_list, padding='longest', return_attention_mask=True, return_tensors='pt')\n","    #taking the [CLS] token\n","    s_out = model(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device))[0]\n","    sentence_encoding = pooled_rep(s_out, enc[\"attention_mask\"].to(device), reduce='mean')\n","    \n","    # processed_facts = [' is '.join(fact_str(x)) for x in tfacts]\n","    fenc = tokenizer.batch_encode_plus(fact_list, padding='longest', return_attention_mask=True, return_tensors='pt')\n","    f_out = model(input_ids=fenc[\"input_ids\"].to(device), attention_mask=fenc[\"attention_mask\"].to(device))[0]\n","    facts_encoding = pooled_rep(f_out, fenc[\"attention_mask\"].to(device), reduce='mean')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0WFfAtl7pnf","executionInfo":{"status":"ok","timestamp":1625887485662,"user_tz":-330,"elapsed":52,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"460e1d5a-4e10-44f3-b639-1c37b8787136"},"source":["scores = F.cosine_similarity(facts_encoding, sentence_encoding[0].unsqueeze(0), 1, 1e-6).cpu().tolist()\n","score_map = {i:v for i,v in enumerate(scores)}\n","score_map"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.4052985906600952, 1: 0.8921797275543213}"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"7uaBDoZy7_06"},"source":["fact_list = [\"I robbed bank\",\"I like dogs\"]\n","sentence_list = [\"मुझे कुत्ते अच्छे लगते हैं\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8GuVxGr3RET","executionInfo":{"status":"ok","timestamp":1625887485666,"user_tz":-330,"elapsed":51,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"b61ca27f-7244-441e-a6f5-32e6da53081b"},"source":["def get_sentence_specific_fact_alignment(tsentences, tfacts, index, threshold=0.5, score_type='cosine', reduce='cls'):\n","    res = []\n","    with torch.no_grad():\n","        enc = tokenizer.batch_encode_plus(tsentences, padding='longest', return_attention_mask=True, return_tensors='pt')\n","        #taking the [CLS] token\n","        s_out = model(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device))[0]\n","        sentence_encoding = pooled_rep(s_out, enc[\"attention_mask\"].to(device), reduce=reduce)\n","        \n","        \n","        processed_facts = [' is '.join(fact_str(x)) for x in tfacts]\n","        fenc = tokenizer.batch_encode_plus(processed_facts, padding='longest', return_attention_mask=True, return_tensors='pt')\n","        f_out = model(input_ids=fenc[\"input_ids\"].to(device), attention_mask=fenc[\"attention_mask\"].to(device))[0]\n","        facts_encoding = pooled_rep(f_out, fenc[\"attention_mask\"].to(device), reduce=reduce)\n","        \n","        scores = F.cosine_similarity(facts_encoding, sentence_encoding[index].unsqueeze(0), 1, 1e-6).cpu().tolist()\n","        if score_type=='cosine':\n","            score_map = {i:v for i,v in enumerate(scores)}\n","        else:\n","            neighbour = 5\n","            sentence_neighbours_enc = F.cosine_similarity(sentence_encoding, sentence_encoding[index].unsqueeze(0))\n","            sentence_scores = [x.item() for i, x in enumerate(sentence_neighbours_enc) if i!=index]\n","            sent_k_score = max(np.sum(sentence_scores[:neighbour]), 0)\n","            sent_k = max(len(sentence_scores[:neighbour]), 1)\n","            \n","            score_map = {}\n","            for i, _ in enumerate(tfacts):\n","                temp_fact_cosine = F.cosine_similarity(facts_encoding, facts_encoding[i].unsqueeze(0))\n","                facts_scores = [x.item() for j, x in enumerate(temp_fact_cosine) if i!=j]\n","                fact_k_score = max(np.sum(facts_scores[:neighbour]), 0)\n","                fact_k = max(len(facts_scores[:neighbour]), 1)\n","                denom = (fact_k_score/(2*fact_k)) + (sent_k_score/(2*sent_k))\n","                score_map[i] = scores[i]/denom\n","        \n","        max_facts = int(threshold*len(score_map))\n","        # print(score_map)\n","        for j, u in sorted(score_map.items(), key=lambda x: x[1], reverse=True):\n","            if max_facts<1:\n","                break\n","            res.append(j)\n","            max_facts-=1\n","        return res\n","index = 0\n","pred_fact_index = get_sentence_specific_fact_alignment(sentence_list, fact_list, index, threshold=0.5, score_type='cosine', reduce='mean')\n","predicted_facts = [fact_list[index] for index in pred_fact_index]\n","print(sentence_list[index])\n","predicted_facts"],"execution_count":null,"outputs":[{"output_type":"stream","text":["मुझे कुत्ते अच्छे लगते हैं\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['I like dogs']"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"WdyQaByZPqEw","cellView":"form","executionInfo":{"status":"error","timestamp":1625887485692,"user_tz":-330,"elapsed":69,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"de833e3a-913e-49da-80c6-e28bd3fb62a2"},"source":["#@title\n","sentence1 = \"I robbed bank\"\n","sentence2 = \"मुझे कुत्ते अच्छे लगते हैं\"\n","# encode sentences to get their embeddings\n","embedding1 = model.encode(sentence1, convert_to_tensor=True)\n","embedding2 = model.encode(sentence2, convert_to_tensor=True)\n","# compute similarity scores of two embeddings\n","cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","print(\"Sentence 1:\", sentence1)\n","print(\"Sentence 2:\", sentence2)\n","print(\"Similarity score:\", cosine_scores.item())"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-61cb00dcf1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"मुझे कुत्ते अच्छे लगते हैं\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# encode sentences to get their embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membedding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0membedding2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# compute similarity scores of two embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1131\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'encode'"]}]},{"cell_type":"markdown","metadata":{"id":"MtnTpTPGGYP3"},"source":["### Loading data from json file"]},{"cell_type":"code","metadata":{"id":"sTxwQR9yGYP4"},"source":["import json \n","# Opening JSON file \n","testA,testC,testP = open('/content/drive/MyDrive/test_data/testA.json',encoding = 'utf-8') ,open('/content/drive/MyDrive/test_data/testC2.json',encoding = 'utf-8'),open('/content/drive/MyDrive/test_data/testP.json',encoding = 'utf-8')\n","dataA,dataC,dataP = json.load(testA),json.load(testC),json.load(testP),\n","# returns JSON object as  \n","# a dictionary \n","# data = json.load(f) \n","# d = {}\n","# d = data\n","  \n","# Closing file \n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUdVuYkfBdY-"},"source":["for qid in dataC:\n","  t =  dataC[qid]\n","  for key in t:\n","    if key == 'triples':\n","      triplelist = t[key]\n","      for l in triplelist:\n","        if len(l) == 1:\n","          triplelist.remove(l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voc83SuKGYP6"},"source":["### Getting a_c_p for getting all the triples\n","# Opening JSON file \n","f = open('/content/drive/MyDrive/a_c_p.json',) \n","data = json.load(f) \n","acp = {}\n","acp = data\n","f.close() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOohNvHhGYP7"},"source":["### Storing sentences and triples from one article\n","Here we store all the sentences and the triples from one article. The article is about the actress 'Kalpana'. We display 5 sentences and 5 triples corresponding to her here."]},{"cell_type":"markdown","metadata":{"id":"FkUpq2A4GYQB"},"source":["### Obtaining sentence and triple embeddings\n","We obtain the sentence embeddings by taking each sentence, obtaining word embeddings for each word in the sentence, and averaging the word embeddings. For triple embeddings , we average the embeddings for each word in the triple."]},{"cell_type":"code","metadata":{"id":"mK9bgHJJGYQC","cellView":"form"},"source":["#@title\n","# Matching Triples with sentences\n","actors_test, cricketers_test, politicians_test = dataA, dataC, dataP\n","actors, cricketers, politicians = actors_test, cricketers_test, politicians_test\n","\n","# Creating the dictionary for the test data where key = sentence and value = list of matching triples\n","actors_test_dict, cricketers_test_dict, politicians_test_dict = {}, {}, {}\n","for l in actors_test:\n","    for k, v in actors_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    actors_test_dict[sentence] = set(t)\n","\n","for l in cricketers_test:\n","    for k, v in cricketers_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    cricketers_test_dict[sentence] = set(t)\n","\n","for l in politicians_test:\n","    for k, v in politicians_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    politicians_test_dict[sentence] = set(t)\n","    \n","len(actors_test_dict),len(cricketers_test_dict),len(politicians_test_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1c1Sh9FGYQE","cellView":"form"},"source":["#@title\n","# We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n","actors_trip, cricketers_trip, politician_trip = [], [], []\n","actors_sent, cricketers_sent, politician_sent = [], [], []\n","\n","# Putting actors,cricketers and politicians from a_c_p\n","act, cric, pol = acp['a'], acp['c'], acp['p']\n","\n","############### Actors #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in actors_test:\n","    eid = actors_test[e]['entity_id']\n","    for ele in act:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = act[ele]['triples']\n","            subject = act[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            actors_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in actors_test:\n","        if eid == actors_test[e]['entity_id']:\n","            sentence = actors_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    actors_sent.append(sentence_list)\n","\n","\n","############### Cricketers #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in cricketers_test:\n","    eid = cricketers_test[e]['entity_id']\n","    for ele in cric:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = cric[ele]['triples']\n","            subject = cric[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            cricketers_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in cricketers_test:\n","        if eid == cricketers_test[e]['entity_id']:\n","            sentence = cricketers_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    cricketers_sent.append(sentence_list)\n","\n","\n","############### Politicians #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in politicians_test:\n","    eid = politicians_test[e]['entity_id']\n","    for ele in pol:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = pol[ele]['triples']\n","            subject = pol[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            politician_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in politicians_test:\n","        if eid == politicians_test[e]['entity_id']:\n","            sentence = politicians_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    politician_sent.append(sentence_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-TnH9IuGYQN"},"source":["len(actors_sent), len(cricketers_sent),len(politician_sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjiX1jl3GYQU"},"source":["len(actors_trip), len(cricketers_trip),len(politician_trip)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1urG_vumGYQW","cellView":"form"},"source":["#@title\n","def matches(sentences,triples):       # Input is a sentence list and triple list for one article\n","    matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","    for sent in sentences:            # For each sent in sentences\n","        ent_matchlist = []\n","        sent_embed = model.encode(sent, convert_to_tensor=True)\n","        for ent in triples:\n","            pred = ent[1]\n","            obj = ent[2]\n","            # print(pred)\n","            ent_embed = model.encode(pred + \" \" + obj, convert_to_tensor=True)\n","            cosine_score = util.pytorch_cos_sim(sent_embed, ent_embed)\n","            similarity = cosine_score.item()\n","            \n","            if similarity > 0.2:\n","              # For evaluation of precision and recall, keep the below 3 lines commented out. They are to append score to matching triples\n","                # ent = list(ent)\n","                # ent.append(similarity)\n","                # ent = tuple(ent)\n","                ent_matchlist.append(ent)\n","        if len(ent_matchlist)>0:    \n","            matches_dict[sent] = set(ent_matchlist)\n","    return matches_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQ0oYdzhGYQX","cellView":"form"},"source":["#@title\n","# #@title\n","#                           ###### Sentence Trasnformer matches ############\n","\n","# matches_act = {}\n","# for sent_list, triple_list in tqdm(zip(actors_sent, actors_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         matches_act[k] = v\n","\n","# matches_cric = {}\n","# for sent_list, triple_list in tqdm(zip(cricketers_sent, cricketers_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         matches_cric[k] = v\n","\n","# matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(politician_sent, politician_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         matches_pol[k] = v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1xhfepWHEzK"},"source":["                      ############ Vanilla transformer matches ##########\n","actors_sent_univ = [slist[0] for slist in actors_sent]\n","cricketers_sent_univ = [slist[0] for slist in cricketers_sent]\n","politicians_sent_univ = [slist[0] for slist in politician_sent]\n","\n","pred_actors_indexes = [ get_sentence_specific_fact_alignment(actors_sent_univ,actors_trip[index],index,\n","                                                                 threshold = 0.5, score_type = 'cosine',\n","                                                                 reduce = 'mean')\n","                        for index in range(len(actors_sent_univ))\n","                      ]\n","predicted_facts = [ list(map(actors_trip[i].__getitem__ ,pred_actors_indexes[i])) for i in range(len(actors_trip))]\n","matches_actors = {i:v for i,v in zip(actors_sent_univ,predicted_facts)}\n","\n","pred_cricketers_indexes = [ get_sentence_specific_fact_alignment(cricketers_sent_univ,cricketers_trip[index],index,\n","                                                                 threshold = 0.5, score_type = 'cosine',\n","                                                                 reduce = 'mean')\n","                        for index in range(len(cricketers_sent_univ))\n","                      ]\n","\n","predicted_facts = [ list(map(cricketers_trip[i].__getitem__ ,pred_cricketers_indexes[i])) for i in range(len(cricketers_trip))]\n","matches_cricketers = {i:v for i,v in zip(cricketers_sent_univ,predicted_facts)}\n","\n","pred_politicians_indexes = [ get_sentence_specific_fact_alignment(politicians_sent_univ,politician_trip[index],index,\n","                                                                 threshold = 0.5, score_type = 'cosine',\n","                                                                 reduce = 'mean')\n","                        for index in range(len(politicians_sent_univ))\n","                      ]\n","predicted_facts = [ list(map(politician_trip[i].__getitem__ ,pred_politicians_indexes[i])) for i in range(len(politician_trip))]\n","matches_politicians = {i:v for i,v in zip(politicians_sent_univ,predicted_facts)}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5ZNCyInGYQY"},"source":["### Evaluation : Precision and Recall"]},{"cell_type":"code","metadata":{"id":"iZHJcNVyGYQY"},"source":["def evaluate(test_dict, matches_dict):\n","    sum_prec = 0\n","    for key, val in matches_dict.items():\n","        tp, fp = 0, 0\n","        for k, v in test_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        if (tp+fp) != 0:\n","            prec = tp/(tp + fp)\n","        else:\n","            prec = 0\n","        sum_prec = prec + sum_prec\n","\n","    sum_rec = 0\n","    for k, v in test_dict.items():\n","        rec = 0\n","        tp, fp = 0, 0\n","        for key, val in matches_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        rec = tp/len(v)\n","        sum_rec = rec + sum_rec\n","\n","    avg_rec, avg_prec = sum_rec/len(test_dict), sum_prec/len(matches_dict)\n","    return avg_rec, avg_prec\n","\n","avg_rec_act, avg_prec_act = evaluate(actors_test_dict, matches_actors)\n","avg_rec_cric, avg_prec_cric = evaluate(cricketers_test_dict, matches_cricketers)\n","avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict, matches_politicianss)\n","# avg_rec_act, avg_prec_act = evaluate(actors_test_dict, matches_act)\n","# avg_rec_cric, avg_prec_cric = evaluate(cricketers_test_dict, matches_cric)\n","# avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict, matches_pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8v5hAzGZFTVG"},"source":["(avg_rec_act,avg_prec_act)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BX28FMolGYQZ"},"source":["(avg_rec_act,avg_prec_act), (avg_rec_cric, avg_prec_cric) , (avg_rec_pol, avg_prec_pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZnxKg-bGYQa","executionInfo":{"status":"ok","timestamp":1625887886712,"user_tz":-330,"elapsed":423,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"eae29686-f9e9-4dd1-85d0-d67812ebb597"},"source":["AverageRecall = (avg_rec_act + avg_rec_cric + avg_rec_pol)/3\n","AveragePrecision = (avg_prec_act + avg_prec_cric + avg_prec_pol)/3\n","\n","AverageRecall, AveragePrecision"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.38040873015873006, 0.21436843603020073)"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"JOrnvHeVM6fd"},"source":["matches_pol"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEC0ANMlGYQa"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"XM6te62fHjPN"},"source":["len(matches_act),len(matches_cric),len(matches_pol)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_3osDlHIOk6"},"source":["So, 7 sentences didn't find any matches at all.Let's see which ones"]},{"cell_type":"code","metadata":{"id":"MW5RmuZpI1gU"},"source":["for sent in politicians_test_dict:\n","  if sent not in matches_pol:\n","    print(sent,\" : \", politicians_test_dict[sent] )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_wNNBsXK7Hh"},"source":["Saving as json"]},{"cell_type":"code","metadata":{"id":"2w2ym6zeNVA6"},"source":["for sent in actors_test_dict:\n","  actors_test_dict[sent] = list(actors_test_dict[sent])\n","for sent in cricketers_test_dict:\n","  cricketers_test_dict[sent] = list(cricketers_test_dict[sent])\n","for sent in politicians_test_dict:\n","  politicians_test_dict[sent] = list(politicians_test_dict[sent])\n","\n","for sent in matches_act:\n","  matches_act[sent] = list(matches_act[sent])\n","for sent in matches_cric:\n","  matches_cric[sent] = list(matches_cric[sent])\n","for sent in matches_pol:\n","  matches_pol[sent] = list(matches_pol[sent])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wziGxMplK-Sh"},"source":["# saving the input test files\n","with open(\"/content//drive//MyDrive//Transformer Output//actors_test.json\", \"w\") as outfile: \n","    json.dump(actors_test_dict,outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/cricketers_test.json\", \"w\") as outfile: \n","    json.dump(cricketers_test_dict, outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/politicians_test.json\", \"w\") as outfile: \n","    json.dump(politicians_test_dict, outfile)\n","\n","#saving the output files\n","with open(\"/content/drive/MyDrive/Transformer Output/actors_matches.json\", \"w\") as outfile: \n","    json.dump(matches_act, outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/cricketers_matches.json\", \"w\") as outfile: \n","    json.dump(matches_cric, outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/politicians_matches.json\", \"w\") as outfile: \n","    json.dump(matches_pol, outfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjE6hl5KGYQb"},"source":["#### We do find quite a few of the matching triples to be relevant to the sentence. But, there are a few irrelevant matches as well.\n","Upon analysis, we think the word overlap is working better than the vector similarity approach. A possible reason can be that when we simply average out the words in a sentence, and when we average out the words in the triples and then find the similarity between these two averages, some semantic information is lost. So, triples that should have been irrelevant are also found as similar. As the word overlap method is a strictly string overlap, the relevance is much stronger."]}]}