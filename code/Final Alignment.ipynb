{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Alignment.ipynb","provenance":[{"file_id":"14rKXCEVCdnNTS0aEoMqe9EPXm0SnxjHi","timestamp":1625818943770},{"file_id":"1d6t3oqMu1gJ3f1dR27YqUA_Dlf_XuzzT","timestamp":1625040094571}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"laesoYf5GYPj"},"source":["# Transformer based models"]},{"cell_type":"markdown","metadata":{"id":"aTJQH4nkGYPz"},"source":["### Importing the required libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qy9CoOqgPeBN","executionInfo":{"status":"ok","timestamp":1625945102698,"user_tz":-330,"elapsed":31374,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"35d3627f-6ac6-4c34-bb16-6f9b550b40f4"},"source":["# ! pip install allennlp\n","# ! pip install allennlp-models\n","# ! pip install nltk==3.5\n","\n","import io\n","import numpy as np\n","# from google_trans_new import google_translator \n","# !pip install mtranslate\n","from scipy.spatial.distance import cosine\n","import json \n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","from nltk import word_tokenize\n","from itertools import groupby \n","from spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI\n","\n","from tqdm import tqdm\n","import pandas as pd\n","\n","!pip install indic-nlp-library\n","import indicnlp\n","from indicnlp.tokenize import indic_tokenize \n","!pip install -U torch\n","import torch\n","!pip install -U transformers\n","!pip install -U sentence-transformers\n","import transformers\n","from sentence_transformers import SentenceTransformer,util\n","import numpy as np\n","# !pip install google_trans_new\n","# from google_trans_new import google_translator\n","!pip install stanfordnlp\n","import stanfordnlp \n","stanfordnlp.download('hi')\n","nlp = stanfordnlp.Pipeline(processors = \"tokenize,pos\",lang = 'hi')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 208M/208M [00:35<00:00, 5.82MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Download complete.  Models saved to: /root/stanfordnlp_resources/hi_hdtb_models.zip\n","Extracting models file for: hi_hdtb\n","Cleaning up...Done.\n","Use device: gpu\n","---\n","Loading: tokenize\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n","---\n","Loading: pos\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb.pretrain.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n","Done loading processors!\n","---\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LERJ60u9mL4a","executionInfo":{"status":"ok","timestamp":1626358783451,"user_tz":-330,"elapsed":24358,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"3d272594-153a-40f0-d630-cd0d4f78abed"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EW9Yu2n0KQV","executionInfo":{"status":"ok","timestamp":1625831351139,"user_tz":-330,"elapsed":40137,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"1a880db8-13bc-4883-9aa6-ee78f5dd2bec"},"source":["from allennlp.predictors import Predictor\n","predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:60: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n","  UserWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s7zQOoUxPpgS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625945180927,"user_tz":-330,"elapsed":539,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"fb13bd9c-be87-4d45-faa3-106e88a4991a"},"source":["# model = SentenceTransformer('stsb-xlm-r-multilingual')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n","# model = SentenceTransformer('msmarco-distilbert-base-v2')\n","# model = SentenceTransformer('LaBSE')\n","if torch.cuda.is_available():\n","  dev = \"cuda:0\"\n","else:\n","  dev = \"cpu\"\n","device = torch.device(dev)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SentenceTransformer(\n","  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n","  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"n8urRp92Zsc5","cellView":"form"},"source":["#@title\n","pos_dict = {\n","'CC': 'coordinating conjunction','CD': 'cardinal digit','DT': 'determiner',\n","'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n","'FW': 'foreign word','IN':  'preposition/subordinating conjunction','JJ': 'adjective \\'big\\'',\n","'JJR': 'adjective, comparative \\'bigger\\'','JJS': 'adjective, superlative \\'biggest\\'',\n","'LS': 'list marker 1)','MD': 'modal could, will','NN': 'noun, singular \\'desk\\'',\n","'NNS': 'noun plural \\'desks\\'','NNP': 'proper noun, singular \\'Harrison\\'',\n","'NNPS': 'proper noun, plural \\'Americans\\'','PDT': 'predeterminer \\'all the kids\\'',\n","'POS': 'possessive ending parent\\'s','PRP': 'personal pronoun I, he, she',\n","'PRP$': 'possessive pronoun my, his, hers','RB': 'adverb very, silently,',\n","'RBR': 'adverb, comparative better','RBS': 'adverb, superlative best',\n","'RP': 'particle give up','TO': 'to go \\'to\\' the store.','UH': 'interjection errrrrrrrm',\n","'VB': 'verb, base form take','VBD': 'verb, past tense took',\n","'VBG': 'verb, gerund/present participle taking','VBN': 'verb, past participle taken',\n","'VBP': 'verb, sing. present, non-3d take','VBZ': 'verb, 3rd person sing. present takes',\n","'WDT': 'wh-determiner which','WP': 'wh-pronoun who, what','WP$': 'possessive wh-pronoun whose',\n","'WRB': 'wh-abverb where, when','QF' : 'quantifier, bahut, thoda, kam (Hindi)','VM' : 'main verb',\n","'PSP' : 'postposition, common in indian langs','DEM' : 'demonstrative, common in indian langs'\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtnTpTPGGYP3"},"source":["### Loading data from json file"]},{"cell_type":"code","metadata":{"id":"sTxwQR9yGYP4"},"source":["import json \n","# Opening JSON file \n","testA,testC,testP = open('/content/drive/MyDrive/test_data/testA.json',encoding = 'utf-8') ,open('/content/drive/MyDrive/test_data/testC2.json',encoding = 'utf-8'),open('/content/drive/MyDrive/test_data/testP.json',encoding = 'utf-8')\n","dataA,dataC,dataP = json.load(testA),json.load(testC),json.load(testP)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUdVuYkfBdY-"},"source":["for qid in dataC:\n","  t =  dataC[qid]\n","  for key in t:\n","    if key == 'triples':\n","      triplelist = t[key]\n","      for l in triplelist:\n","        if len(l) == 1:\n","          triplelist.remove(l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voc83SuKGYP6"},"source":["### Getting a_c_p for getting all the triples\n","# Opening JSON file \n","f = open('/content/drive/MyDrive/a_c_p.json',) \n","data = json.load(f) \n","acp = {}\n","acp = data\n","f.close() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVmHUNcYN8_3"},"source":["# Sentence to Triple Matching"]},{"cell_type":"markdown","metadata":{"id":"FkUpq2A4GYQB"},"source":["### Obtaining sentence and triple embeddings\n","We obtain the sentence embeddings by taking each sentence, obtaining word embeddings for each word in the sentence, and averaging the word embeddings. For triple embeddings , we average the embeddings for each word in the triple."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mK9bgHJJGYQC","executionInfo":{"status":"ok","timestamp":1626069409251,"user_tz":-330,"elapsed":436,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"df09e1d1-be62-4225-e70b-947b3c672914"},"source":["# Matching Triples with sentences\n","actors_test, cricketers_test, politicians_test = dataA, dataC, dataP\n","actors, cricketers, politicians = actors_test, cricketers_test, politicians_test\n","\n","# Creating the dictionary for the test data where key = sentence and value = list of matching triples\n","actors_test_dict, cricketers_test_dict, politicians_test_dict = {}, {}, {}\n","for l in actors_test:\n","    for k, v in actors_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    actors_test_dict[sentence] = set(t)\n","\n","for l in cricketers_test:\n","    for k, v in cricketers_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    cricketers_test_dict[sentence] = set(t)\n","\n","for l in politicians_test:\n","    for k, v in politicians_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    politicians_test_dict[sentence] = set(t)\n","# for l in politicians_test:\n","#   entries = politicians_test[l]\n","#   subject = entries['personLabel']\n","#   triple_list = entries['triples']\n","  \n","    \n","len(actors_test_dict),len(cricketers_test_dict),len(politicians_test_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 100)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"6k6v3OMmhxKQ"},"source":["actors_test_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"POvZ8P9LZowa","executionInfo":{"status":"error","timestamp":1626069353977,"user_tz":-330,"elapsed":447,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"bc26a898-a0f4-4e01-fbd6-4161d3b9a3bf"},"source":["import json\n","# Making the test_dict\n","test_dict = {}\n","final_sent, final_trip = [],[]\n","with open('/content/drive/MyDrive/test_data/final data/final_test.jsonl', 'r', encoding='utf8') as read_file:\n","    for line in read_file:\n","        data = json.loads(line)\n","        test_dict[data[\"sentence\"]] = data['correct_triples']\n","        final_sent.append([data[\"sentence\"]])\n","        tuple_trip_list = []\n","        for triple in data['all_triples']:\n","            tuple_trip_list.append(tuple(triple))\n","        final_trip.append(tuple_trip_list)\n","        #processing on json data dictionary\n","        pass\n","len(test_dict), len(final_sent), len(final_trip)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-21a648372e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_trip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/test_data/final data/final_test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/test_data/final data/final_test.jsonl'"]}]},{"cell_type":"code","metadata":{"id":"kuOFZGuQlqro"},"source":["final_trip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJAOisIkJ-E-"},"source":["Keeping only the gold standard Sentences in politcians"]},{"cell_type":"markdown","metadata":{"id":"VIfca8mnNbYF"},"source":["Cleaning the annotated triples --  We need to prune out certain triples in politicians -- like date of birth, data of death in predicate, British India and Dominion of India in objects."]},{"cell_type":"code","metadata":{"id":"zQLXOHmbieCI"},"source":["                    ################ Cleaning test_dict ##################\n","temp = {}\n","for sentence in test_dict:\n","  triplelist = test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      triple = tuple(triple)\n","      new_triple_list.add(triple)\n","  temp[sentence] = new_triple_list\n","test_dict = temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"934QMdeSfZid"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bK1OXiTjNVcM"},"source":["#@title\n","temp = {}\n","for sentence in politicians_test_dict:\n","  triplelist = politicians_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","politicians_test_dict = temp\n","\n","temp = {}\n","for sentence in actors_test_dict:\n","  triplelist = actors_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","actors_test_dict = temp\n","\n","temp = {}\n","for sentence in cricketers_test_dict:\n","  triplelist = cricketers_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","cricketers_test_dict = temp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivQ_xQkBPGyt"},"source":["Loading ACP.json for training"]},{"cell_type":"code","metadata":{"id":"cHka8MKOYDmx"},"source":["f = open('/content/drive/MyDrive/test_data/politicians.json',encoding = 'utf-8')\n","acp_p = json.load(f)\n","f = open('/content/drive/MyDrive/test_data/actors.json',encoding = 'utf-8')\n","acp_a = json.load(f)\n","f = open('/content/drive/MyDrive/test_data/cricketers.json',encoding = 'utf-8')\n","acp_c = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"-1c1Sh9FGYQE"},"source":["#@title\n","# #@title\n","# # We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n","# actors_trip, cricketers_trip, politician_trip = [], [], []\n","# actors_sent, cricketers_sent, politician_sent = [], [], []\n","\n","# # Putting actors,cricketers and politicians from a_c_p\n","# # act, cric, pol = acp['a'], acp['c'], acp['p']      # Original acp.json\n","\n","# # Now, we are using better, cleaned , pruned version of acp json, ( cleaned by Shivprasad)\n","# act = acp_a\n","# cric = acp_c\n","# pol = acp_p\n","# ############### Actors #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in actors_test:\n","#     eid = actors_test[e]['entity_id']\n","#     for ele in act:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = act[ele]['triples']\n","#             subject = act[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             actors_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in actors_test:\n","#         if eid == actors_test[e]['entity_id']:\n","#             sentence = actors_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     actors_sent.append(sentence_list)\n","\n","\n","# ############### Cricketers #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in cricketers_test:\n","#     eid = cricketers_test[e]['entity_id']\n","#     for ele in cric:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = cric[ele]['triples']\n","#             subject = cric[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             cricketers_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in cricketers_test:\n","#         if eid == cricketers_test[e]['entity_id']:\n","#             sentence = cricketers_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     cricketers_sent.append(sentence_list)\n","\n","\n","# ############### Politicians #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in politicians_test:\n","#     eid = politicians_test[e]['entity_id']\n","#     for ele in pol:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = pol[ele]['triples']\n","#             subject = pol[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             politician_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in politicians_test:\n","#         if eid == politicians_test[e]['entity_id']:\n","#             sentence = politicians_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     politician_sent.append(sentence_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiPxAFIbQZag"},"source":["#@title\n","# We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n","actors_trip, cricketers_trip, politician_trip = [], [], []\n","actors_sent, cricketers_sent, politician_sent = [], [], []\n","\n","# Putting actors,cricketers and politicians from a_c_p\n","# act, cric, pol = acp['a'], acp['c'], acp['p']      # Original acp.json\n","\n","# Now, we are using better, cleaned , pruned version of acp json, ( cleaned by Shivprasad)\n","act = acp_a\n","cric = acp_c\n","pol = acp_p\n","############### Actors #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in actors_test:\n","    eid = actors_test[e]['entity_id']\n","    for ele in act:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = act[ele]['triples']\n","            subject = act[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip[1]\n","                obj = trip[2]\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            actors_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","actors_train_sent = []    ##Group train sentences in list of lists\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in actors_test:\n","        if eid == actors_test[e]['entity_id']:\n","            sentence = actors_test[e]['sentence']\n","            sentence_list.append(sentence)\n","            train_sentence = act[eid]['sentences']\n","    actors_sent.append(sentence_list)\n","    actors_train_sent.append(train_sentence)\n","######     Keyphrase  ##################\n","# 1. Make it so that sentences (in training data act) are grouped together. \n","############### Cricketers #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","\n","entity_tracking = []\n","for e in cricketers_test:\n","    eid = cricketers_test[e]['entity_id']\n","    for ele in cric:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = cric[ele]['triples']\n","            subject = cric[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            cricketers_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","cricketers_train_sent = []\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in cricketers_test:\n","        if eid == cricketers_test[e]['entity_id']:\n","            sentence = cricketers_test[e]['sentence']\n","            sentence_list.append(sentence)\n","            train_sentence = acp['c'][eid]['sentences']\n","    cricketers_sent.append(sentence_list)\n","    cricketers_train_sent.append(train_sentence)\n","\n","############### Politicians #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in politicians_test:\n","    eid = politicians_test[e]['entity_id']\n","    for ele in pol:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = pol[ele]['triples']\n","            subject = pol[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            politician_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","politicians_train_sent = []\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in politicians_test:\n","        if eid == politicians_test[e]['entity_id']:\n","            sentence = politicians_test[e]['sentence']\n","            sentence_list.append(sentence)\n","            train_sentence = pol[eid]['sentences']\n","    politician_sent.append(sentence_list)\n","    politicians_train_sent.append(train_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqVhOIhVCoDn","executionInfo":{"status":"ok","timestamp":1625945259024,"user_tz":-330,"elapsed":1109,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"a6dd75a3-073b-41e3-f7a9-ba49a1691b22"},"source":["len(actors_train_sent),len(cricketers_train_sent),len(politicians_train_sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-TnH9IuGYQN","executionInfo":{"status":"ok","timestamp":1625945263883,"user_tz":-330,"elapsed":419,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"69e7f5fa-bfed-4ea0-87af-5f2d9f8b12e3"},"source":["len(actors_sent), len(cricketers_sent),len(politician_sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjiX1jl3GYQU","executionInfo":{"status":"ok","timestamp":1625945267860,"user_tz":-330,"elapsed":471,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"9acf60f3-7e65-4c72-d064-6a06e364a8f8"},"source":["len(actors_trip), len(cricketers_trip),len(politician_trip)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"iPU6dglank-8"},"source":["# MUSE (Nearest Neighbours functions)"]},{"cell_type":"code","metadata":{"id":"mWrvmHean5_g"},"source":["def load_vec(emb_path, nmax=50000):\n","    vectors = []\n","    word2id = {}\n","    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n","        next(f)\n","        for i, line in enumerate(f):\n","            word, vect = line.rstrip().split(' ', 1)\n","            vect = np.fromstring(vect, sep=' ')\n","            assert word not in word2id, 'word found twice'\n","            vectors.append(vect)\n","            word2id[word] = len(word2id)\n","            if len(word2id) == nmax:\n","                break\n","    id2word = {v: k for k, v in word2id.items()}\n","    embeddings = np.vstack(vectors)\n","    return embeddings, id2word, word2id\n","\n","import nltk\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n","    # print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n","    word2id = {v: k for k, v in src_id2word.items()}\n","    targetwordlist = []                               # List of target words for the source word \n","    if word in word2id:                               #Check if word is in vocab\n","      word_emb = src_emb[word2id[word]]\n","      scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n","      k_best = scores.argsort()[-K:][::-1]\n","      for i, idx in enumerate(k_best):\n","          # print((scores[idx], tgt_id2word[idx]))      #To give both distance and word\n","          if tgt_id2word[idx] not in stop_words:\n","            targetwordlist.append(tgt_id2word[idx].lower())\n","      return targetwordlist\n","    else:\n","      translate_text = translate(word,\"en\",\"auto\") \n","      transw = translate_text\n","      if transw not in stop_words:\n","        # print(word,\" - Translated - \",transw)\n","        return [transw.lower()]\n","src_path = '/content/drive/My Drive/wiki.hi.align.vec'\n","tgt_path = '/content/drive/My Drive/wiki.en.align.vec'\n","nmax = 50000  # maximum number of word embeddings to load\n","\n","src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n","tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jdOhFCjDMPO"},"source":["!pip install mtranslate\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"PvMr9UlP7VSL","executionInfo":{"status":"ok","timestamp":1625830971888,"user_tz":-330,"elapsed":450,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"00287d01-d333-4530-c19d-d70bf43cfc63"},"source":["from mtranslate import translate\n","translate('kamla',\"en\",\"auto\")\n","# translate_text = translator.translate('कमला',lang_tgt='en')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'kamla'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"Qs1-KrR6o9Vo"},"source":["# Alignment (NER based filtering)"]},{"cell_type":"code","metadata":{"id":"VGf4X4wwpC9-"},"source":["def matches(sentences,triples):       # Input is a sentence list and triple list for one article\n","    matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","    for sent in sentences:            # For each sent in sentences\n","        ent_matchlist = []\n","        hindiwords = [w for w in indic_tokenize.trivial_tokenize(sent, lang = 'hi') if w not in STOP_WORDS_HI]\n","        sent_no_stop = ' '.join(hindiwords)  # Making the snetence without stop words\n","        sent_embed = model.encode(sent_no_stop, convert_to_tensor=True)\n","        bucket = []\n","        # Creating word overlap bucket\n","        for word in hindiwords:\n","                if word.isdigit():\n","                  continue\n","                else:\n","                  if len(word) > 2:\n","                    wordlist = get_nn(word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n","                    if(wordlist):\n","                      bucket.extend(wordlist)\n","\n","\n","        for ent in triples:\n","            subj = ent[0]\n","            pred = ent[1]\n","            obj = ent[2]\n","            # print(pred)\n","\n","            # Finding named entities from triples\n","            results = predictor.predict(sentence=pred + \" \" + obj)\n","            named_entities = [word for word, tag in zip(results[\"words\"], results[\"tags\"]) if tag!='O']\n","            # named_entities = []\n","            # prev = \"\"\n","            # for word, tag in zip(results[\"words\"], results[\"tags\"]):\n","            #   if tag !='O':\n","            #     prev = prev + word + \" \"\n","            #   else:\n","            #     if prev !='':\n","            #       named_entities.append(prev.strip(' '))\n","            #     prev = \"\"\n","            # if prev!='':\n","            #   named_entities.append(prev.strip(' '))  \n","            flag = 0\n","            if named_entities:\n","              # If named entities exist in the triple\n","              named_entities = set([ele.lower() for ele in named_entities])\n","\n","              if len(set(bucket).intersection(named_entities)) / len(named_entities) >= 0.75:\n","                flag = 1\n","                # print(sent,named_entities)\n","            if named_entities and flag == 0:\n","              continue\n","            # ent_embed_1 = model.encode(subj + \" \" + pred + \" \" + obj, convert_to_tensor=True)\n","            ent_embed_2 = model.encode(\"He/She is \"+ pred + \" \" + obj, convert_to_tensor=True)\n","            # cosine_score_1 = util.pytorch_cos_sim(sent_embed, ent_embed_1)\n","            cosine_score_2 = util.pytorch_cos_sim(sent_embed, ent_embed_2)\n","            # similarity_1 = cosine_score_1.item()\n","            similarity_2 = cosine_score_2.item()\n","            # similarity = max(similarity_1, similarity_2)\n","            similarity = similarity_2\n","            if similarity > 0.19:\n","                # print(subj + \" \" + pred + \" \" + obj + \" == \", similarity_1 , \", \", similarity_2)\n","              # For evaluation of precision and recall, keep the below 3 lines commented out. They are to append score to matching triples\n","                # ent = list(ent)\n","                # ent.append(similarity)\n","                # ent = tuple(ent)\n","                ent_matchlist.append(ent)\n","        if len(ent_matchlist)>0:    \n","            matches_dict[sent] = set(ent_matchlist)\n","    return matches_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EthIcgjUGUng"},"source":["# Keyphrase Ranking"]},{"cell_type":"code","metadata":{"id":"fSn1tn1qGYja"},"source":["# We have obtained the ordered train list of lists (in same order as annotated sentences)\n","# Now, maintaining the same order , we create a train list of list of keyphrases\n","from nltk import RegexpParser\n","\n","def extract_pos(doc):\n","    pos = [(wrd.text,wrd.pos) for sent in doc.sentences for wrd in sent.words]\n","    return pos\n","def get_kp(sent):\n","  hindi = nlp(sent)\n","  extract_pos(hindi)\n","\n","  pos_tags = extract_pos(hindi)\n","  patterns = \"NP: {<JJ>*<NN|NNS|NNC|NNCS|NNP|NNPS|NNPC|NNPCS>+}\"\n","  PChunker = RegexpParser(patterns)\n","  output = PChunker.parse(pos_tags)\n","  keyphrases = []\n","  for subtree in output.subtrees(filter=lambda t: t.label() == 'NP'):\n","      keyphrases.append((' '.join([x[0] for x in subtree])))\n","  return keyphrases\n","\n","def rank_keyphrases(total_keyphrases,individual_keyphrases):\n","  article_keyphrases = ' '.join(total_keyphrases)\n","  article_keyphrases_embedding = model.encode(article_keyphrases,convert_to_tensor= True)\n","  individual_embeddings_of_keyphrases = model.encode(individual_keyphrases,convert_to_tensor= True)\n","  keyphrase_scores = []\n","  for keyphrase,embedding in zip(total_keyphrases,individual_embeddings_of_keyphrases):\n","    cosine_scores = util.pytorch_cos_sim(article_keyphrases_embedding, embedding)\n","    keyphrase_scores.append([cosine_scores.item(),keyphrase])\n","  keyphrase_scores.sort(reverse=True)\n","  return keyphrase_scores\n","  # for score,kp in keyphrase_scores:\n","  #   print(kp,\" : \",score)\n","\n","### Getting Total Keyphrases (from train sentences) ###\n","actors_total_keyphrases,cricketers_total_keyphrases,politicians_total_keyphrases = [],[],[]\n","for sentence_list in actors_train_sent:\n","  total_keyphrases = []\n","  for sentence in sentence_list:\n","    keyphrase = get_kp(sentence)\n","    total_keyphrases.extend(keyphrase)\n","  actors_total_keyphrases.append(total_keyphrases)\n","\n","for sentence_list in cricketers_train_sent:\n","  total_keyphrases = []\n","  for sentence in sentence_list:\n","    keyphrase = get_kp(sentence)\n","    total_keyphrases.extend(keyphrase)\n","  cricketers_total_keyphrases.append(total_keyphrases)\n","\n","for sentence_list in politicians_train_sent:\n","  total_keyphrases = []\n","  for sentence in sentence_list:\n","    keyphrase = get_kp(sentence)\n","    total_keyphrases.extend(keyphrase)\n","  politicians_total_keyphrases.append(total_keyphrases)\n","\n","### Getting Individual Keyphrases (from test sentences) ###\n","actors_individual_keyphrases,cricketers_individual_keyphrases,politicians_individual_keyphrases = [],[],[]\n","for sentence_list in actors_sent:\n","  total_sents = \"\"\n","  for sentence in sentence_list:\n","    total_sents = total_sents + sentence\n","  keyphrases = get_kp(total_sents)\n","  actors_individual_keyphrases.append(keyphrases)\n","\n","for sentence_list in cricketers_sent:\n","  total_sents = \"\"\n","  for sentence in sentence_list:\n","    total_sents = total_sents + sentence\n","  keyphrases = get_kp(total_sents)\n","  cricketers_individual_keyphrases.append(keyphrases)\n","\n","for sentence_list in politician_sent:\n","  total_sents = \"\"\n","  for sentence in sentence_list:\n","    total_sents = total_sents + sentence\n","  keyphrases = get_kp(total_sents)\n","  politicians_individual_keyphrases.append(keyphrases)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esX1rl7pS95r","executionInfo":{"status":"ok","timestamp":1625123756014,"user_tz":-330,"elapsed":462,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"8fa620a8-b275-44bc-9481-490e4c118af8"},"source":["len(actors_total_keyphrases),len(cricketers_total_keyphrases),len(politicians_total_keyphrases)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKw11gKUXF0L","executionInfo":{"status":"ok","timestamp":1625123759165,"user_tz":-330,"elapsed":459,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"1c776cab-d1ed-4505-a767-4028fd2a06e0"},"source":["len(actors_individual_keyphrases),len(cricketers_individual_keyphrases),len(politicians_individual_keyphrases)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"cZusgdhNTKXy"},"source":["actors_keyphrase_scores, cricketers_keyphrase_scores, politicians_keyphrase_scores = [],[],[]\n","for total_keyphrases,individual_keyphrases in zip(actors_total_keyphrases,actors_individual_keyphrases):\n","  keyphrase_scores = rank_keyphrases(total_keyphrases,individual_keyphrases)\n","  actors_keyphrase_scores.append(keyphrase_scores)\n","\n","for total_keyphrases,individual_keyphrases in zip(cricketers_total_keyphrases,cricketers_individual_keyphrases):\n","  keyphrase_scores = rank_keyphrases(total_keyphrases,individual_keyphrases)\n","  cricketers_keyphrase_scores.append(keyphrase_scores)\n","\n","for total_keyphrases,individual_keyphrases in zip(politicians_total_keyphrases,politicians_individual_keyphrases):\n","  keyphrase_scores = rank_keyphrases(total_keyphrases,individual_keyphrases)\n","  politicians_keyphrase_scores.append(keyphrase_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAMfWZ9oYEEs","executionInfo":{"status":"ok","timestamp":1625123778155,"user_tz":-330,"elapsed":469,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"9656d525-24c0-4659-945b-b97f4ebd6592"},"source":["len(actors_keyphrase_scores), len(cricketers_keyphrase_scores), len(politicians_keyphrase_scores)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, 100, 99)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"sFTBBXUzOvSx"},"source":["# Alignment (keyphrase)"]},{"cell_type":"markdown","metadata":{"id":"BmPPpuvTrPqz"},"source":["Finding Top K matches"]},{"cell_type":"code","metadata":{"id":"vw37ylbAuTDu"},"source":["from nltk.util import ngrams\n","from nltk import RegexpParser\n","\n","def extract_pos(doc):\n","    pos = [(wrd.text,wrd.pos) for sent in doc.sentences for wrd in sent.words]\n","    return pos\n","\n","def preprocess(triple_list):\n","  new_triple_list = []\n","  for s,p,o in triple_list:\n","    if p == 'date of birth' or p == 'date of death' or 'name' in p or p == 'sex or gender' or o == 'British India' or o == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.append((s,p,o))\n","  return new_triple_list\n","\n","def matches_kp(sentences,triples):       # Matches based on keyphrase\n","    matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","    for sent in sentences:\n","      sentence2 =  sent\n","      hindi = nlp(sentence2)\n","      extract_pos(hindi)\n","      pos_tags = extract_pos(hindi)\n","      patterns = \"NP: {<JJ>*<NN|NNS|NNC|NNCS|NNP|NNPS|NNPC|NNPCS>+}\"\n","      # patterns = \"NP:{(<JJ>* <NN.*>+ <PSP>)? <JJ>* <NN.*>+}\"\n","      PChunker = RegexpParser(patterns)\n","      pos_tags = extract_pos(hindi)\n","      output = PChunker.parse(pos_tags)\n","      keyphrases = []\n","      for subtree in output.subtrees(filter=lambda t: t.label() == 'NP'):\n","          keyphrases.append((' '.join([x[0] for x in subtree])))\n","      triple_list = triples\n","      triple_list = preprocess(triple_list)\n","      matches = []\n","      # keyphrases = ['ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है']\n","      for kp in keyphrases:\n","        words = [w for w in indic_tokenize.trivial_tokenize(kp, lang = 'hi') if w not in STOP_WORDS_HI]\n","        k = len(words)\n","        bigrams = ngrams(words,2)\n","        trigrams = ngrams(words,3)\n","        if len(words) == 1:\n","          bigrams,trigrams = [(words)],[(words)]\n","        matches_for_a_keyphrase = []\n","        for tup in bigrams:\n","          sbi = ' '.join(list(tup))\n","          for s,p,o in triple_list:\n","            embedding1 = model.encode(sbi, convert_to_tensor=True)\n","            embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","            cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","            if cosine_scores.item() > 0.45:\n","              matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),kp])\n","        for tup in trigrams:\n","          sbi = ' '.join(list(tup))\n","          for s,p,o in triple_list:\n","            embedding1 = model.encode(sbi, convert_to_tensor=True)\n","            embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","            cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","            if cosine_scores.item() > 0.45:\n","              matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),kp])\n","        \n","        matches_for_a_keyphrase.sort(reverse = True)\n","        triple_matches = []\n","        # Now, we have to keep only (s,p,o) from matches_for_a_keyphrase in triple_matches\n","        for _,triple,_ in matches_for_a_keyphrase:\n","          triple_matches.append(triple) \n","        matches.extend(triple_matches[:k+1])\n","        \n","      matches.sort(reverse = True)\n","      matches_dict[sent] = list(set(matches))   \n","    return matches_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBs8RtqToY5Z"},"source":["# Execute"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQ0oYdzhGYQX","executionInfo":{"status":"ok","timestamp":1625946493056,"user_tz":-330,"elapsed":79463,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"9fe6a056-9b4c-4e02-d490-12186252f9a6"},"source":["# from tqdm import tqdm\n","# matches_final_NER = {}\n","# for sent_list, triple_list in tqdm(zip(final_sent, final_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         matches_final_NER[k] = v\n","# matches_keyphrases = {}\n","# for sent_list, triple_list in tqdm(zip(final_sent, final_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_keyphrases[k] = v\n","\n","# matches_pol = {}\n","# for sent_list, triple_list, keyphrase_scores in tqdm(zip(politician_sent, politician_trip,politicians_keyphrase_scores)):\n","#     for k, v in matches_kp(sent_list, triple_list, keyphrase_scores).items():\n","#         matches_pol[k] = v\n","\n","\n","# pol_matches_sent = [list(matches_pol.keys())]\n","# pol_matches_trips = list(matches_pol.values())\n","# ner_matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(politician_sent, politician_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         ner_matches_pol[k] = v\n","\n","# ner_matches_act = {}\n","# for sent_list, triple_list in tqdm(zip(actors_sent, actors_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         ner_matches_act[k] = v\n","\n","matches_act = {}\n","for sent_list, triple_list in tqdm(zip(actors_sent, actors_trip)):\n","    for k, v in matches_kp(sent_list, triple_list).items():\n","        matches_act[k] = v\n","\n","# matches_cric = {}\n","# for sent_list, triple_list, keyphrase_scores in tqdm(zip(cricketers_sent, cricketers_trip, cricketers_keyphrase_scores)):\n","#     for k, v in matches_kp(sent_list, triple_list,keyphrase_scores).items():\n","#         matches_cric[k] = v\n","\n","# matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(pol_sent, pol_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_pol[k] = v"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50it [01:19,  1.58s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VCO2mIlgXqnd"},"source":["## post processing\n","for sent in matches_keyphrases:\n","  triples = matches_keyphrases[sent]\n","  new_trips = []\n","  for triple in triples:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      pass\n","    else:\n","      new_trips.append(triple)\n","  matches_keyphrases[sent] = new_trips\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5ZNCyInGYQY"},"source":["### Evaluation : Precision and Recall"]},{"cell_type":"code","metadata":{"id":"fefHrcbHCOVX"},"source":["politicians_test_dict_corrected = json.load(open('/content/drive/MyDrive/test_data/corrected_annotated_data/politicians_test(corrected).json',encoding = 'utf-8'))\n","for sent in politicians_test_dict_corrected:\n","  triplist = politicians_test_dict_corrected[sent]\n","  tup = set()\n","  for s,p,o in triplist:\n","    tup.add((s,p,o))\n","  politicians_test_dict_corrected[sent] = tup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZHJcNVyGYQY"},"source":["def evaluate(test_dict, matches_dict):\n","    sum_prec = 0\n","    for key, val in matches_dict.items():\n","        tp, fp = 0, 0\n","        for k, v in test_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        if (tp+fp) != 0:\n","            prec = tp/(tp + fp)\n","        else:\n","            prec = 0\n","        sum_prec = prec + sum_prec\n","\n","    sum_rec = 0\n","    for k, v in test_dict.items():\n","        rec = 0\n","        tp, fp = 0, 0\n","        for key, val in matches_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        if len(v) == 0:\n","          rec = 1\n","        else:\n","          rec = tp/len(v)\n","        sum_rec = rec + sum_rec\n","\n","    avg_rec, avg_prec = sum_rec/len(test_dict), sum_prec/len(matches_dict)\n","    return avg_rec, avg_prec\n","\n","# avg_rec_act, avg_prec_act = evaluate(test_dict, matches_keyphrases)\n","avg_rec_act, avg_prec_act = evaluate(actors_test_dict, matches_act)\n","# avg_rec_cric, avg_prec_cric = evaluate(cricketers_test_dict, matches_cric)\n","# avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict_corrected, ner_matches_pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XULcBYdLB4AO"},"source":["politicians_test_dict_corrected"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhT8DK_wHtzj"},"source":["politicians_keyphrase_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXrw0D3jYU5C"},"source":["def global_evaluate(test_dict, matches_dict):\n","    sum_tp,sum_fp = 0,0\n","    total_count = 0\n","    for key, val in matches_dict.items():\n","        tp, fp = 0, 0\n","        for k, v in test_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        sum_tp += tp\n","        sum_fp += fp\n","\n","    \n","    for k, v in test_dict.items():\n","        total_count += len(v)\n","    \n","    prec = sum_tp/(sum_tp+sum_fp)\n","    rec = sum_tp/(total_count)\n","    return rec,prec\n","avg_rec_act, avg_prec_act = global_evaluate(actors_test_dict, matches_act)\n","# avg_rec_cric, avg_prec_cric = global_evaluate(cricketers_test, cricketers_matches)\n","# avg_rec_pol, avg_prec_pol = global_evaluate(politicians_test_dict_corrected, politicians_matches)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8v5hAzGZFTVG","executionInfo":{"status":"ok","timestamp":1625946509704,"user_tz":-330,"elapsed":418,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"f91e4856-cf6f-463c-d0af-4a036308d941"},"source":["(avg_rec_act,avg_prec_act)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.46923809523809523, 0.6779523809523811)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BX28FMolGYQZ","executionInfo":{"status":"ok","timestamp":1625124857278,"user_tz":-330,"elapsed":500,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"2a58bddd-e9f7-49f0-be32-8b9bbbfbc7d6"},"source":["(avg_rec_act,avg_prec_act), (avg_rec_cric, avg_prec_cric) , (avg_rec_pol, avg_prec_pol)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((0.7159863945578233, 0.6646190476190478),\n"," (0.7880471380471382, 0.7918095238095237),\n"," (0.7877104377104377, 0.6549302549302548))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZnxKg-bGYQa","executionInfo":{"status":"ok","timestamp":1625124864294,"user_tz":-330,"elapsed":453,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"c37e997a-4ff7-4d1f-b928-77bbd78dfeea"},"source":["AverageRecall = (avg_rec_act + avg_rec_cric + avg_rec_pol)/3\n","AveragePrecision = (avg_prec_act + avg_prec_cric + avg_prec_pol)/3\n","\n","AverageRecall, AveragePrecision"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7639146567717998, 0.7037862754529421)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"FtUJcKUmglIr"},"source":["politicians_test_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEC0ANMlGYQa"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"XM6te62fHjPN"},"source":["len(matches_act),len(matches_cric),len(matches_pol)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_3osDlHIOk6"},"source":["So, 7 sentences didn't find any matches at all.Let's see which ones"]},{"cell_type":"code","metadata":{"id":"MW5RmuZpI1gU"},"source":["for sent in politicians_test_dict:\n","  if sent not in matches_pol:\n","    print(sent,\" : \", politicians_test_dict[sent] )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_wNNBsXK7Hh"},"source":["Saving as json"]},{"cell_type":"code","metadata":{"id":"2w2ym6zeNVA6"},"source":["# for sent in actors_test_dict:\n","#   actors_test_dict[sent] = list(actors_test_dict[sent])\n","# for sent in cricketers_test_dict:\n","#   cricketers_test_dict[sent] = list(cricketers_test_dict[sent])\n","# for sent in politicians_test_dict:\n","#   politicians_test_dict[sent] = list(politicians_test_dict[sent])\n","\n","# for sent in matches_act:\n","#   matches_act[sent] = list(matches_act[sent])\n","# for sent in matches_cric:\n","#   matches_cric[sent] = list(matches_cric[sent])\n","for sent in matches_pol:\n","  matches_pol[sent] = list(matches_pol[sent])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyew2idcnigA"},"source":["test_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wziGxMplK-Sh","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1625840095582,"user_tz":-330,"elapsed":435,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"d7ca6889-2183-4c74-b7b6-e75c32da0749"},"source":["# # saving the input test files\n","# with open(\"/content//drive//MyDrive//Transformer Output//actors_test.json\", \"w\") as outfile: \n","#     json.dump(actors_test_dict,outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/cricketers_test.json\", \"w\") as outfile: \n","#     json.dump(cricketers_test_dict, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/politicians_test.json\", \"w\") as outfile: \n","#     json.dump(politicians_test_dict, outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/test_dict.json\", \"w\") as outfile: \n","    json.dump(test_dict, outfile)\n","\n","# #saving the output files\n","# with open(\"/content/drive/MyDrive/Transformer Output/actors_matches.json\", \"w\") as outfile: \n","#     json.dump(matches_act, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/cricketers_matches.json\", \"w\") as outfile: \n","#     json.dump(matches_cric, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/matches_final_NER.json\", \"w\") as outfile: \n","#     json.dump(matches_final_NER, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/matches_keyphrases.json\", \"w\") as outfile: \n","#     json.dump(matches_keyphrases, outfile)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-8399b734b3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     json.dump(politicians_test_dict, outfile)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Transformer Output/test_dict.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# #saving the output files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Object of type set is not JSON serializable"]}]},{"cell_type":"markdown","metadata":{"id":"hjE6hl5KGYQb"},"source":["#### We do find quite a few of the matching triples to be relevant to the sentence. But, there are a few irrelevant matches as well.\n","Upon analysis, we think the word overlap is working better than the vector similarity approach. A possible reason can be that when we simply average out the words in a sentence, and when we average out the words in the triples and then find the similarity between these two averages, some semantic information is lost. So, triples that should have been irrelevant are also found as similar. As the word overlap method is a strictly string overlap, the relevance is much stronger."]},{"cell_type":"code","metadata":{"id":"3RocdMa5Qo43"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d29tzmUFQn_B"},"source":["# Aligned corpus stats"]},{"cell_type":"code","metadata":{"id":"t-c57WJ9Yl6F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626358816178,"user_tz":-330,"elapsed":8790,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"a083c96d-cba4-4d72-fbee-50cad56a6fbb"},"source":["!pip install indic-nlp-library\n","import indicnlp\n","from indicnlp.tokenize import indic_tokenize "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting indic-nlp-library\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/d4/495bb43b88a2a6d04b09c29fc5115f24872af74cd8317fe84026abd4ddb1/indic_nlp_library-0.81-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 40kB 4.1MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n","Collecting sphinx-rtd-theme\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/24/2475e8f83519b54b2148d4a56eb1111f9cec630d088c3ffc214492c12107/sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1MB)\n","\u001b[K     |████████████████████████████████| 9.2MB 10.6MB/s \n","\u001b[?25hCollecting sphinx-argparse\n","  Downloading https://files.pythonhosted.org/packages/06/2b/dfad6a1831c3aeeae25d8d3d417224684befbf45e10c7f2141631616a6ed/sphinx-argparse-0.2.5.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n","Collecting morfessor\n","  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n","Collecting docutils<0.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/44/8a15e45ffa96e6cf82956dd8d7af9e666357e16b0d93b253903475ee947f/docutils-0.16-py2.py3-none-any.whl (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 26.7MB/s \n","\u001b[?25hRequirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (1.8.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (2.23.0)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (1.2.4)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (2.6.1)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (2.11.3)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (0.7.12)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (2.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (57.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (20.9)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (1.2.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->sphinx-rtd-theme->indic-nlp-library) (2.9.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx->sphinx-rtd-theme->indic-nlp-library) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx->sphinx-rtd-theme->indic-nlp-library) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx->sphinx-rtd-theme->indic-nlp-library) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx->sphinx-rtd-theme->indic-nlp-library) (2021.5.30)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->sphinx-rtd-theme->indic-nlp-library) (1.1.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx->sphinx-rtd-theme->indic-nlp-library) (2.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx->sphinx-rtd-theme->indic-nlp-library) (2.4.7)\n","Building wheels for collected packages: sphinx-argparse\n","  Building wheel for sphinx-argparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sphinx-argparse: filename=sphinx_argparse-0.2.5-cp37-none-any.whl size=11552 sha256=e14080c5002514317cd7728104c83286c2b16dff05a46e82a59943684ed0ef3a\n","  Stored in directory: /root/.cache/pip/wheels/2a/18/1b/4990a1859da4edc77ab312bc2986c08d2733fb5713d06e44f5\n","Successfully built sphinx-argparse\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: docutils, sphinx-rtd-theme, sphinx-argparse, morfessor, indic-nlp-library\n","  Found existing installation: docutils 0.17.1\n","    Uninstalling docutils-0.17.1:\n","      Successfully uninstalled docutils-0.17.1\n","Successfully installed docutils-0.16 indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.2.5 sphinx-rtd-theme-0.5.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yT70wLhkQsAF","executionInfo":{"status":"ok","timestamp":1626358820145,"user_tz":-330,"elapsed":3991,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}}},"source":["import json\n","\n","actors = json.load(open('/content/drive/MyDrive/Aligned corpus/final_actors.json',encoding = 'utf-8'))\n","# aligned_actors = json.load(actors)\n","\n","cricketers = json.load(open('/content/drive/MyDrive/Aligned corpus/final_cricketers.json',encoding = 'utf-8'))\n","# aligned_cricketers = json.load(cricketers)\n","\n","politicians = json.load(open('/content/drive/MyDrive/Aligned corpus/final_politicians.json',encoding = 'utf-8'))\n","\n","singers = json.load(open('/content/drive/MyDrive/Aligned corpus/final_singers.json',encoding = 'utf-8'))\n","\n","writers = json.load(open('/content/drive/MyDrive/Aligned corpus/final_writers.json',encoding = 'utf-8'))\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MUNzd1XxhkN","executionInfo":{"status":"ok","timestamp":1626359061832,"user_tz":-330,"elapsed":449,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"347008d0-3349-487b-ac53-8db4d8bd3051"},"source":["actors_with_facts = {i:v for i,v in actors.items() if len(v)>=1}\n","cricketers_with_facts = {i:v for i,v in cricketers.items() if len(v)>=1}\n","politicians_with_facts = {i:v for i,v in politicians.items() if len(v)>=1}\n","singers_with_facts = {i:v for i,v in singers.items() if len(v)>=1}\n","writers_with_facts = {i:v for i,v in writers.items() if len(v)>=1}\n","len(actors_with_facts),len(cricketers_with_facts),len(politicians_with_facts),len(singers_with_facts), len(writers_with_facts)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2742, 4455, 5674, 940, 3778)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygx3oVs9YqGq","executionInfo":{"status":"ok","timestamp":1625992468574,"user_tz":-330,"elapsed":542,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"313fa55e-52c2-4ca9-dd5f-5e94d94c3be2"},"source":["sents = list(actors.keys())\n","len(indic_tokenize.trivial_tokenize(sents[0], lang = 'hi'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"Da5nZlg1iPQQ","executionInfo":{"status":"ok","timestamp":1626358939683,"user_tz":-330,"elapsed":1184,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}}},"source":["import json\n","journalists_test_dict = json.load(open('/content/drive/MyDrive/test_data/processed_test_data/journlists_test_dict.json'))\n","singers_test_dict = json.load(open('/content/drive/MyDrive/test_data/processed_test_data/singers_test_dict.json'))\n","writers_test_dict = json.load(open('/content/drive/MyDrive/test_data/processed_test_data/writers_test_dict.json'))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gU14fWr8TnLU","executionInfo":{"status":"ok","timestamp":1626358945303,"user_tz":-330,"elapsed":3,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}}},"source":["        # stats\n","import numpy as np\n","def stats(entities):\n","  sentences = list(entities.keys())\n","  print()\n","  print(\"Total sentences : \", len(sentences))\n","  len_of_sents = np.array([len(indic_tokenize.trivial_tokenize(s, lang = 'hi')) for s in sentences])\n","  mean = np.mean(len_of_sents)\n","  print(\"Mean length of sents : \", mean)\n","  median = np.median(len_of_sents)\n","  print(\"Median length of sents : \", median)\n","  min = int(np.min(len_of_sents))\n","  print(\"Min length of sent : \", min)\n","  max = int(np.max(len_of_sents))\n","  print(\"Max length of sent : \", max)\n","  print()\n","  print(\"############# Facts #########\")\n","  print()\n","  facts = list(entities.values())\n","  len_of_facts = np.array([len(f) for f in facts])\n","  mean = np.mean(len_of_facts)\n","  print(\"Mean length of facts : \", mean)\n","  median = np.median(len_of_facts)\n","  print(\"Median length of facts : \", median)\n","  min = int(np.min(len_of_facts))\n","  print(\"Min length of fact : \", min)\n","  max = int(np.max(len_of_facts))\n","  print(\"Max length of fact : \", max)\n","  print(\"===============================\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFnIbYckXJST","executionInfo":{"status":"ok","timestamp":1626359114881,"user_tz":-330,"elapsed":1148,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"}},"outputId":"ba406edd-4810-4510-e604-9d31f5b24bc9"},"source":["print(\"Actors ---\")\n","stats(actors_with_facts)\n","print()\n","print(\"Cricketers ---\")\n","stats(cricketers_with_facts)\n","print()\n","print(\"Politicians ---\")\n","stats(politicians_with_facts)\n","print()\n","print(\"Singers ---\")\n","stats(singers_with_facts)\n","print()\n","print(\"Writers ---\")\n","stats(writers_with_facts)\n","print()\n","\n","########### Alignment stats for test annotated data ###########\n","print()\n","print(\"     Test annotated stats\")\n","# print()\n","# print(\"-------------------Actors -----------\")\n","# stats(actors_test_dict)\n","# print()\n","# print(\"-------------------Cricketers -------\")\n","# stats(cricketers_test_dict)\n","# print(\"-------------------Politicians -------\")\n","# stats(politicians_test_dict)\n","# print()\n","print(\"-------------------Writer -----------\")\n","stats(writers_test_dict)\n","print()\n","print(\"-------------------Singers -------\")\n","stats(singers_test_dict)\n","print(\"-------------------Journalists -------\")\n","stats(journalists_test_dict)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Actors ---\n","\n","Total sentences :  2742\n","Mean length of sents :  17.65754923413567\n","Median length of sents :  16.0\n","Min length of sent :  4\n","Max length of sent :  68\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.324580598103574\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  8\n","===============================\n","\n","Cricketers ---\n","\n","Total sentences :  4455\n","Mean length of sents :  20.467340067340068\n","Median length of sents :  19.0\n","Min length of sent :  2\n","Max length of sent :  74\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.305050505050505\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  8\n","===============================\n","\n","Politicians ---\n","\n","Total sentences :  5674\n","Mean length of sents :  18.057102573140643\n","Median length of sents :  16.0\n","Min length of sent :  3\n","Max length of sent :  75\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.723123017271766\n","Median length of facts :  3.0\n","Min length of fact :  1\n","Max length of fact :  9\n","===============================\n","\n","Singers ---\n","\n","Total sentences :  940\n","Mean length of sents :  18.786170212765956\n","Median length of sents :  17.0\n","Min length of sent :  4\n","Max length of sent :  72\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.4627659574468086\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  8\n","===============================\n","\n","Writers ---\n","\n","Total sentences :  3778\n","Mean length of sents :  17.83668607728957\n","Median length of sents :  18.0\n","Min length of sent :  2\n","Max length of sent :  73\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.475913181577554\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  8\n","===============================\n","\n","\n","     Test annotated stats\n","-------------------Writer -----------\n","\n","Total sentences :  47\n","Mean length of sents :  15.659574468085106\n","Median length of sents :  16.0\n","Min length of sent :  6\n","Max length of sent :  26\n","\n","############# Facts #########\n","\n","Mean length of facts :  1.7872340425531914\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  4\n","===============================\n","\n","-------------------Singers -------\n","\n","Total sentences :  25\n","Mean length of sents :  18.04\n","Median length of sents :  14.0\n","Min length of sent :  8\n","Max length of sent :  40\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.92\n","Median length of facts :  3.0\n","Min length of fact :  1\n","Max length of fact :  5\n","===============================\n","-------------------Journalists -------\n","\n","Total sentences :  25\n","Mean length of sents :  17.32\n","Median length of sents :  18.0\n","Min length of sent :  5\n","Max length of sent :  35\n","\n","############# Facts #########\n","\n","Mean length of facts :  2.12\n","Median length of facts :  2.0\n","Min length of fact :  1\n","Max length of fact :  4\n","===============================\n"],"name":"stdout"}]}]}