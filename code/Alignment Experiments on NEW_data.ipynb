{"cells":[{"cell_type":"markdown","metadata":{"id":"laesoYf5GYPj"},"source":["# Transformer based models"]},{"cell_type":"markdown","metadata":{"id":"aTJQH4nkGYPz"},"source":["### Importing the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"qy9CoOqgPeBN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/4b/e2cd1576fb8016eddd1b63f045abe7c3227a9864fad58a6a864633af9f67/allennlp-2.5.0-py3-none-any.whl (681kB)\n","\u001b[K     |████████████████████████████████| 686kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: requests\u003e=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n","Collecting boto3\u003c2.0,\u003e=1.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c0/8af2139d5658eccde11f45fd9d27046edb286fd60f5371e27870612287bd/boto3-1.17.111-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 16.1MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n","Requirement already satisfied: tqdm\u003e=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n","Collecting tensorboardX\u003e=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/0b/a26bbe92667c549d39c40b80c5ddec638fbae9521f04aeef26560e07e504/tensorboardX-2.4-py2.py3-none-any.whl (124kB)\n","\u001b[K     |████████████████████████████████| 133kB 15.8MB/s \n","\u001b[?25hCollecting torch\u003c1.9.0,\u003e=1.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n","\u001b[K     |████████████████████████████████| 804.1MB 23kB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n","Collecting checklist==0.0.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/b3/8511f50025b9fc66f5feacf9eb2db044c321f4026b6937cb3820f29e9c1d/checklist-0.0.11.tar.gz (12.1MB)\n","\u001b[K     |████████████████████████████████| 12.1MB 268kB/s \n","\u001b[?25hCollecting huggingface-hub\u003e=0.0.8\n","  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n","Collecting google-cloud-storage\u003c1.39.0,\u003e=1.38.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/75/78ed0d1ef691592b94e7a3d9f58153298166486342a97df82d3c5b66cc16/google_cloud_storage-1.38.0-py2.py3-none-any.whl (103kB)\n","\u001b[K     |████████████████████████████████| 112kB 54.1MB/s \n","\u001b[?25hRequirement already satisfied: spacy\u003c3.1,\u003e=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n","Collecting jsonnet\u003e=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n","\u001b[K     |████████████████████████████████| 266kB 56.7MB/s \n","\u001b[?25hCollecting overrides==3.1.0\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.8.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 38.7MB/s \n","\u001b[?25hCollecting wandb\u003c0.11.0,\u003e=0.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/f6/91c07f54c2162854f5028aaa13f576ca17a3bc0cf6da02c2ad5baddae128/wandb-0.10.33-py2.py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 41.9MB/s \n","\u001b[?25hCollecting torchvision\u003c0.10.0,\u003e=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n","\u001b[K     |████████████████████████████████| 17.4MB 131kB/s \n","\u001b[?25hCollecting transformers\u003c4.7,\u003e=4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 24.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock\u003c3.1,\u003e=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp) (2021.5.30)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk-\u003eallennlp) (1.15.0)\n","Requirement already satisfied: py\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp) (1.10.0)\n","Requirement already satisfied: pluggy\u003c0.8,\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp) (0.7.1)\n","Requirement already satisfied: atomicwrites\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp) (1.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp) (57.0.0)\n","Requirement already satisfied: attrs\u003e=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp) (21.2.0)\n","Collecting s3transfer\u003c0.5.0,\u003e=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n","\u001b[?25hCollecting jmespath\u003c1.0.0,\u003e=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting botocore\u003c1.21.0,\u003e=1.20.111\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/56/64570ac92c7cb88ad731dea4da4a83d3edc9f00a13a969ad826354ba5a58/botocore-1.20.111.tar.gz (7.9MB)\n","\u001b[K     |████████████████████████████████| 7.9MB 37.3MB/s \n","\u001b[?25hRequirement already satisfied: cached-property; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py-\u003eallennlp) (1.5.2)\n","Requirement already satisfied: protobuf\u003e=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX\u003e=1.2-\u003eallennlp) (3.17.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003c1.9.0,\u003e=1.6.0-\u003eallennlp) (3.7.4.3)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003eallennlp) (1.0.1)\n","Collecting munch\u003e=2.5\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: dill\u003e=0.3.1 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp) (0.3.4)\n","Requirement already satisfied: jupyter\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp) (1.0.0)\n","Requirement already satisfied: ipywidgets\u003e=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp) (7.6.3)\n","Collecting patternfork-nosql\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/c9/44df2e48530ff9ebdc0f5a916831aecef2cf10806f3021f09cb4a5040674/patternfork_nosql-3.6.tar.gz (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 141kB/s \n","\u001b[?25hCollecting iso-639\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8d/27969852f4e664525c3d070e44b2b719bc195f4d18c311c52e57bb93614e/iso-639-0.4.5.tar.gz (167kB)\n","\u001b[K     |████████████████████████████████| 174kB 49.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.8-\u003eallennlp) (4.6.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.8-\u003eallennlp) (20.9)\n","Collecting google-cloud-core\u003c2.0dev,\u003e=1.4.1\n","  Downloading https://files.pythonhosted.org/packages/88/a7/b74266a6fd888d91a6f517c574c17425731181fe44e2df1e414d4b77fbe8/google_cloud_core-1.7.1-py2.py3-none-any.whl\n","Collecting google-resumable-media\u003c2.0dev,\u003e=1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/96/4360dc70bef5559b3faf3deeda97aae7d10ff7660d41fd233eb792e7d09f/google_resumable_media-1.3.1-py2.py3-none-any.whl (75kB)\n","\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n","\u001b[?25hRequirement already satisfied: google-auth\u003c2.0dev,\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (1.32.1)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (0.8.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (7.4.0)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (3.0.5)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (1.0.0)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (1.0.5)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (1.0.5)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (1.1.3)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (2.0.5)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp) (0.4.1)\n","Collecting shortuuid\u003e=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Collecting GitPython\u003e=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n","\u001b[K     |████████████████████████████████| 174kB 56.2MB/s \n","\u001b[?25hCollecting configparser\u003e=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n","Collecting docker-pycreds\u003e=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp) (3.13)\n","Requirement already satisfied: python-dateutil\u003e=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp) (2.8.1)\n","Collecting pathtools\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp) (5.4.8)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp) (7.1.2)\n","Requirement already satisfied: promise\u003c3,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp) (2.3)\n","Collecting subprocess32\u003e=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n","\u001b[?25hCollecting sentry-sdk\u003e=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/41/75fad31fff378871c462745ce724b3701a6acad17028d79476ec2545e40f/sentry_sdk-1.3.0-py2.py3-none-any.whl (133kB)\n","\u001b[K     |████████████████████████████████| 143kB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: pillow\u003e=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision\u003c0.10.0,\u003e=0.8.1-\u003eallennlp) (7.1.2)\n","Collecting tokenizers\u003c0.11,\u003e=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 37.5MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 44.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c4.7,\u003e=4.1-\u003eallennlp) (2019.12.20)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.1.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.6.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (4.10.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (3.5.1)\n","Requirement already satisfied: ipython\u003e=4.0.0; python_version \u003e= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (5.5.0)\n","Requirement already satisfied: traitlets\u003e=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (5.0.5)\n","Requirement already satisfied: jupyterlab-widgets\u003e=1.0.0; python_version \u003e= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (1.0.0)\n","Requirement already satisfied: nbformat\u003e=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (5.1.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp) (0.16.0)\n","Collecting backports.csv\n","  Downloading https://files.pythonhosted.org/packages/8e/26/a6bd68f13e0f38fbb643d6e497fc3462be83a0b6c4d43425c78bb51a7291/backports.csv-1.0.7-py2.py3-none-any.whl\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp) (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp) (4.2.6)\n","Collecting feedparser\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/15bf6781a861bbc5dd801d467f26448fb322bfedcd30f2e62b148d104dfb/feedparser-6.0.8-py3-none-any.whl (81kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n","\u001b[?25hCollecting pdfminer.six\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 3.3MB/s \n","\u001b[?25hCollecting python-docx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 13.9MB/s \n","\u001b[?25hCollecting cherrypy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/e333e597c090d12d3a0d208f751366da7db8c44c8b392b467dd993366e53/CherryPy-18.6.1-py2.py3-none-any.whl (419kB)\n","\u001b[K     |████████████████████████████████| 430kB 47.1MB/s \n","\u001b[?25hRequirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003ehuggingface-hub\u003e=0.0.8-\u003eallennlp) (3.4.1)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.9-\u003ehuggingface-hub\u003e=0.0.8-\u003eallennlp) (2.4.7)\n","Requirement already satisfied: google-api-core\u003c2.0.0dev,\u003e=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (1.26.3)\n","Collecting google-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\"\n","  Downloading https://files.pythonhosted.org/packages/fc/ae/b6efa1019e18c6c791f0f5cd93b2ff40f8f06696dbf04db39ec0f5591b1e/google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (4.2.2)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4; python_version \u003e= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (4.7.2)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (0.2.8)\n","Collecting gitdb\u003c5,\u003e=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n","\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (4.7.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.2.0)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (1.9.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (2.6.1)\n","Requirement already satisfied: jupyter-client\u003e=4.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.3.5)\n","Requirement already satisfied: pyzmq\u003e=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (22.1.0)\n","Requirement already satisfied: tornado\u003e=4 in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (5.1.1)\n","Requirement already satisfied: terminado\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.10.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (1.7.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (2.11.3)\n","Requirement already satisfied: pandocfilters\u003e=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (1.4.3)\n","Requirement already satisfied: entrypoints\u003e=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.5.0)\n","Requirement already satisfied: mistune\u003c2,\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (3.3.0)\n","Requirement already satisfied: prompt-toolkit\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (1.0.18)\n","Requirement already satisfied: simplegeneric\u003e0.8 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (4.4.2)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (0.7.5)\n","Requirement already satisfied: jsonschema!=2.5.0,\u003e=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat\u003e=4.2.0-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp) (2.6.0)\n","Collecting sgmllib3k\n","  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n","Collecting cryptography\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 30.4MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp) (2.4.0)\n","Collecting jaraco.collections\n","  Downloading https://files.pythonhosted.org/packages/d5/1a/a0d6861d2aca6df92643c755966c8a60e40353e4c5e7a5c2f4e5ed733817/jaraco.collections-3.3.0-py3-none-any.whl\n","Collecting portend\u003e=2.1.1\n","  Downloading https://files.pythonhosted.org/packages/b8/a1/fd29409cced540facdd29abb986d988cb1f22c8170d10022ea73af77fa55/portend-2.7.1-py3-none-any.whl\n","Collecting zc.lockfile\n","  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n","Collecting cheroot\u003e=8.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/95/86fe6480af78fea7b0e7e1bf02e6acd4cb9e561ea200bd6d6e1398fe5426/cheroot-8.5.2-py2.py3-none-any.whl (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 13.0MB/s \n","\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core\u003c2.0.0dev,\u003e=1.21.0-\u003egoogle-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (2018.9)\n","Requirement already satisfied: googleapis-common-protos\u003c2.0dev,\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core\u003c2.0.0dev,\u003e=1.21.0-\u003egoogle-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (1.53.0)\n","Requirement already satisfied: cffi\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\"-\u003egoogle-resumable-media\u003c2.0dev,\u003e=1.2.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (1.14.5)\n","Requirement already satisfied: pyasn1\u003e=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa\u003c5,\u003e=3.1.4; python_version \u003e= \"3.6\"-\u003egoogle-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (0.4.8)\n","Collecting smmap\u003c5,\u003e=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado\u003e=0.8.1-\u003enotebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.7.0)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003enotebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (2.0.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-\u003enbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.5.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit\u003c2.0.0,\u003e=1.0.0-\u003ejupyter-console-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp) (0.2.5)\n","Collecting jaraco.text\n","  Downloading https://files.pythonhosted.org/packages/c1/74/2a3c4835c079df16db8a9c50263eebb0125849fee5b16de353a059b7545d/jaraco.text-3.5.0-py3-none-any.whl\n","Collecting jaraco.classes\n","  Downloading https://files.pythonhosted.org/packages/b8/74/bee5fc11594974746535117546404678fc7b899476e769c3c55bc0cfaa02/jaraco.classes-3.2.1-py3-none-any.whl\n","Collecting tempora\u003e=1.8\n","  Downloading https://files.pythonhosted.org/packages/58/6e/928b4726ee2762efea4a84e8f5e73fb46b396636a5ec260d6274a1de24d5/tempora-4.1.1-py3-none-any.whl\n","Collecting jaraco.functools\n","  Downloading https://files.pythonhosted.org/packages/b5/da/e51e7b58c8fe132990edd1e3ef25bcd9801eb7f91d0f642ac7f8d97e4a36/jaraco.functools-3.3.0-py3-none-any.whl\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.0.0-\u003egoogle-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\"-\u003egoogle-resumable-media\u003c2.0dev,\u003e=1.2.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp) (2.20)\n","Building wheels for collected packages: checklist, jsonnet, overrides, botocore, patternfork-nosql, iso-639, pathtools, subprocess32, python-docx, sgmllib3k\n","  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for checklist: filename=checklist-0.0.11-cp37-none-any.whl size=12165633 sha256=33df04052ef4dbb941d5d00f51eaa89db4ffec6502f539b07464e624426f77f3\n","  Stored in directory: /root/.cache/pip/wheels/28/5b/df/c24f900588532f3fc08e71bafae579fbed3102efe28b00e5cd\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388708 sha256=03a11314d0ab4111c1e9ebc0d5299884be40170f046679aff8f29945aafd1d96\n","  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10187 sha256=99647a00b770484980768b625e5f4ac9cef7b661f0418134074e4d1f2dbb6def\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","  Building wheel for botocore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for botocore: filename=botocore-1.20.111-py2.py3-none-any.whl size=7695032 sha256=5e3831ee71243e6157a1a765283824b84e85e6bd80fa49377074a4fcb7a81c53\n","  Stored in directory: /root/.cache/pip/wheels/1e/5f/dd/9021b3f78dc76c95f97ea9cd1798aa6da9bc1a61fe7d1bb9fa\n","  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-cp37-none-any.whl size=22332808 sha256=dd8ceb42f21d0acb0589fb1de5557a7cc640085c15567a5ab0d5d0b303fbf26f\n","  Stored in directory: /root/.cache/pip/wheels/dc/0d/ae/060a851f2104f4cc79380cc57d89f29d77a239597eeecfcf4d\n","  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iso-639: filename=iso_639-0.4.5-cp37-none-any.whl size=169063 sha256=261ce9107ad036ef06aaf1cce4e5a048f064d5873d3cd6f06b9f08597d348575\n","  Stored in directory: /root/.cache/pip/wheels/52/60/07/73aed7d23ae9b5729970632922ed5e45b535bcd4b8df77ebe9\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=febdcaabafb89afd7f771879da9f379fb6877061579113d10a54d07eaff4ba9b\n","  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=6b1f75fc388c1ff4f5f0db0a488dab83c22b01666e225f4314ff6d95569bf667\n","  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-cp37-none-any.whl size=184508 sha256=1c39fbfe18a51a8ae57d1da5e4a555f40d5a631b4c61b597d1631d1e9112b2df\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/f1/a7cb70b38633ae04e7fb963b1c70f63fd6fc01c075b8230adc\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=0d1f306589a3a5633b8c930fbd3723aaeebd36a464e21ff7aabccf8ee6fa047c\n","  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n","Successfully built checklist jsonnet overrides botocore patternfork-nosql iso-639 pathtools subprocess32 python-docx sgmllib3k\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-cloud-bigquery 1.21.0 has requirement google-resumable-media!=0.4.0,\u003c0.5.0dev,\u003e=0.3.1, but you'll have google-resumable-media 1.3.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: botocore 1.20.111 has requirement urllib3\u003c1.27,\u003e=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: transformers 4.6.1 has requirement huggingface-hub==0.0.8, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, tensorboardX, torch, munch, tokenizers, sacremoses, huggingface-hub, transformers, backports.csv, sgmllib3k, feedparser, cryptography, pdfminer.six, python-docx, jaraco.functools, jaraco.text, jaraco.classes, jaraco.collections, tempora, portend, zc.lockfile, cheroot, cherrypy, patternfork-nosql, iso-639, checklist, google-cloud-core, google-crc32c, google-resumable-media, google-cloud-storage, jsonnet, overrides, sentencepiece, shortuuid, smmap, gitdb, GitPython, configparser, docker-pycreds, pathtools, subprocess32, sentry-sdk, wandb, torchvision, allennlp\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Found existing installation: google-cloud-core 1.0.3\n","    Uninstalling google-cloud-core-1.0.3:\n","      Successfully uninstalled google-cloud-core-1.0.3\n","  Found existing installation: google-resumable-media 0.4.1\n","    Uninstalling google-resumable-media-0.4.1:\n","      Successfully uninstalled google-resumable-media-0.4.1\n","  Found existing installation: google-cloud-storage 1.18.1\n","    Uninstalling google-cloud-storage-1.18.1:\n","      Successfully uninstalled google-cloud-storage-1.18.1\n","  Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","Successfully installed GitPython-3.1.18 allennlp-2.5.0 backports.csv-1.0.7 boto3-1.17.111 botocore-1.20.111 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 cryptography-3.4.7 docker-pycreds-0.4.0 feedparser-6.0.8 gitdb-4.0.7 google-cloud-core-1.7.1 google-cloud-storage-1.38.0 google-crc32c-1.1.2 google-resumable-media-1.3.1 huggingface-hub-0.0.13 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 jmespath-0.10.0 jsonnet-0.17.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.11 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.96 sentry-sdk-1.3.0 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tempora-4.1.1 tensorboardX-2.4 tokenizers-0.10.3 torch-1.8.1 torchvision-0.9.1 transformers-4.6.1 wandb-0.10.33 zc.lockfile-2.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Collecting allennlp-models\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/d9/222a8ff52a07ddf7b6662e60acc385699630351342e7b0baf87532460c66/allennlp_models-2.5.0-py3-none-any.whl (433kB)\n","\u001b[K     |████████████████████████████████| 440kB 9.4MB/s \n","\u001b[?25hCollecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n","\u001b[?25hCollecting word2number\u003e=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Requirement already satisfied: allennlp\u003c2.6,\u003e=2.5.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.5.0)\n","Collecting py-rouge==1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003c1.9.0,\u003e=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.8.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n","Collecting conllu==4.4\n","  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy-\u003eallennlp-models) (0.2.5)\n","Requirement already satisfied: wandb\u003c0.11.0,\u003e=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.10.33)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.4.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (8.8.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.6.4)\n","Requirement already satisfied: boto3\u003c2.0,\u003e=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.17.111)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.1.0)\n","Requirement already satisfied: huggingface-hub\u003e=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.0.13)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.1.96)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.99)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.22.2.post1)\n","Requirement already satisfied: spacy\u003c3.1,\u003e=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.2.4)\n","Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.1.0)\n","Requirement already satisfied: filelock\u003c3.1,\u003e=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.0.12)\n","Requirement already satisfied: transformers\u003c4.7,\u003e=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.6.1)\n","Requirement already satisfied: torchvision\u003c0.10.0,\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.9.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.1.0)\n","Requirement already satisfied: tensorboardX\u003e=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.4)\n","Requirement already satisfied: requests\u003e=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.23.0)\n","Requirement already satisfied: jsonnet\u003e=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.17.0)\n","Requirement already satisfied: google-cloud-storage\u003c1.39.0,\u003e=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.38.0)\n","Requirement already satisfied: tqdm\u003e=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.41.1)\n","Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.0.11)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003c1.9.0,\u003e=1.7.0-\u003eallennlp-models) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk-\u003eallennlp-models) (1.15.0)\n","Requirement already satisfied: GitPython\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.1.18)\n","Requirement already satisfied: subprocess32\u003e=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.5.4)\n","Requirement already satisfied: shortuuid\u003e=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.1)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (7.1.2)\n","Requirement already satisfied: promise\u003c3,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.3)\n","Requirement already satisfied: python-dateutil\u003e=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.8.1)\n","Requirement already satisfied: sentry-sdk\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.3.0)\n","Requirement already satisfied: configparser\u003e=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.0.2)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.1.2)\n","Requirement already satisfied: protobuf\u003e=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.17.3)\n","Requirement already satisfied: docker-pycreds\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.4.0)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.4.8)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.13)\n","Requirement already satisfied: py\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (57.0.0)\n","Requirement already satisfied: atomicwrites\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.4.0)\n","Requirement already satisfied: pluggy\u003c0.8,\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.7.1)\n","Requirement already satisfied: attrs\u003e=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (21.2.0)\n","Requirement already satisfied: s3transfer\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3\u003c2.0,\u003e=1.14-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.4.2)\n","Requirement already satisfied: jmespath\u003c1.0.0,\u003e=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3\u003c2.0,\u003e=1.14-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.10.0)\n","Requirement already satisfied: botocore\u003c1.21.0,\u003e=1.20.111 in /usr/local/lib/python3.7/dist-packages (from boto3\u003c2.0,\u003e=1.14-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.20.111)\n","Requirement already satisfied: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.8-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.6.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.8-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (20.9)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.1)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.4.1)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.8.2)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (7.4.0)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.0.5)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.0)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.1.3)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.5)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.1,\u003e=2.1.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.0.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c4.7,\u003e=4.1-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers\u003c4.7,\u003e=4.1-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.0.45)\n","Requirement already satisfied: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c4.7,\u003e=4.1-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.10.3)\n","Requirement already satisfied: pillow\u003e=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision\u003c0.10.0,\u003e=0.8.1-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (7.1.2)\n","Requirement already satisfied: cached-property; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.5.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.18-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2021.5.30)\n","Requirement already satisfied: google-auth\u003c2.0dev,\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.32.1)\n","Requirement already satisfied: google-resumable-media\u003c2.0dev,\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.3.1)\n","Requirement already satisfied: google-cloud-core\u003c2.0dev,\u003e=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.7.1)\n","Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.4.5)\n","Requirement already satisfied: jupyter\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.0)\n","Requirement already satisfied: dill\u003e=0.3.1 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.3.4)\n","Requirement already satisfied: munch\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.5.0)\n","Requirement already satisfied: ipywidgets\u003e=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (7.6.3)\n","Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.6)\n","Requirement already satisfied: gitdb\u003c5,\u003e=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython\u003e=1.0.0-\u003ewandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.0.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003ehuggingface-hub\u003e=0.0.8-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.4.1)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.9-\u003ehuggingface-hub\u003e=0.0.8-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.4.7)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.2.8)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.2.2)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4; python_version \u003e= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.7.2)\n","Requirement already satisfied: google-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\" in /usr/local/lib/python3.7/dist-packages (from google-resumable-media\u003c2.0dev,\u003e=1.2.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.1.2)\n","Requirement already satisfied: google-api-core\u003c2.0.0dev,\u003e=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.26.3)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.2.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.6.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.3.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.10.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.1.1)\n","Requirement already satisfied: nbformat\u003e=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.1.3)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.5.1)\n","Requirement already satisfied: ipython\u003e=4.0.0; python_version \u003e= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.5.0)\n","Requirement already satisfied: jupyterlab-widgets\u003e=1.0.0; python_version \u003e= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.0)\n","Requirement already satisfied: traitlets\u003e=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.0.5)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.8.11)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.2.6)\n","Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.7)\n","Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (18.6.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.6.3)\n","Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (6.0.8)\n","Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (20201018)\n","Requirement already satisfied: smmap\u003c5,\u003e=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython\u003e=1.0.0-\u003ewandb\u003c0.11.0,\u003e=0.10.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.0.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c2.0dev,\u003e=1.11.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.4.8)\n","Requirement already satisfied: cffi\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\"-\u003egoogle-resumable-media\u003c2.0dev,\u003e=1.2.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.14.5)\n","Requirement already satisfied: googleapis-common-protos\u003c2.0dev,\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core\u003c2.0.0dev,\u003e=1.21.0-\u003egoogle-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.53.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core\u003c2.0.0dev,\u003e=1.21.0-\u003egoogle-cloud-core\u003c2.0dev,\u003e=1.4.1-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2018.9)\n","Requirement already satisfied: prompt-toolkit\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.18)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from jupyter-console-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.3.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from jupyter-console-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.6.1)\n","Requirement already satisfied: entrypoints\u003e=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.3.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.7.1)\n","Requirement already satisfied: mistune\u003c2,\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.8.4)\n","Requirement already satisfied: jinja2\u003e=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.11.3)\n","Requirement already satisfied: pandocfilters\u003e=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.5.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.2.0)\n","Requirement already satisfied: terminado\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.10.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.7.1)\n","Requirement already satisfied: tornado\u003e=4 in /usr/local/lib/python3.7/dist-packages (from notebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (5.1.1)\n","Requirement already satisfied: pyzmq\u003e=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (22.1.0)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.9.0)\n","Requirement already satisfied: jsonschema!=2.5.0,\u003e=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat\u003e=4.2.0-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.6.0)\n","Requirement already satisfied: simplegeneric\u003e0.8 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.4.2)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython\u003e=4.0.0; python_version \u003e= \"3.3\"-\u003eipywidgets\u003e=7.5-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.7.5)\n","Requirement already satisfied: portend\u003e=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.7.1)\n","Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.0)\n","Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.3.0)\n","Requirement already satisfied: cheroot\u003e=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (8.5.2)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (1.0.0)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.4.7)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.0.0-\u003egoogle-crc32c\u003c2.0dev,\u003e=1.0; python_version \u003e= \"3.5\"-\u003egoogle-resumable-media\u003c2.0dev,\u003e=1.2.0-\u003egoogle-cloud-storage\u003c1.39.0,\u003e=1.38.0-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.20)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-\u003enbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.5.1)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2\u003e=2.4-\u003enbconvert-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (2.0.1)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado\u003e=0.8.1-\u003enotebook-\u003ejupyter\u003e=1.0-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (0.7.0)\n","Requirement already satisfied: tempora\u003e=1.8 in /usr/local/lib/python3.7/dist-packages (from portend\u003e=2.1.1-\u003echerrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (4.1.1)\n","Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections-\u003echerrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.2.1)\n","Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections-\u003echerrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.5.0)\n","Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot\u003e=8.2.1-\u003echerrypy-\u003epatternfork-nosql-\u003echecklist==0.0.11-\u003eallennlp\u003c2.6,\u003e=2.5.0-\u003eallennlp-models) (3.3.0)\n","Building wheels for collected packages: ftfy, word2number\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=62d918c55389b2c927b2acf984b224a6f42082e33fb132a21abc4ca8018ea01d\n","  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5584 sha256=28252595b740fc763d793651d3f696d82159b6d6c4e8fb7ed9b1a4b85b157a89\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","Successfully built ftfy word2number\n","Installing collected packages: ftfy, word2number, py-rouge, conllu, allennlp-models\n","Successfully installed allennlp-models-2.5.0 conllu-4.4 ftfy-6.0.3 py-rouge-1.1 word2number-1.1\n","Collecting nltk==3.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.0.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.41.1)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434694 sha256=3e259c452a7b86f51cab755762eb87d8c66cf73a25362e635fde3fb90d801725\n","  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.5\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting indic-nlp-library\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/d4/495bb43b88a2a6d04b09c29fc5115f24872af74cd8317fe84026abd4ddb1/indic_nlp_library-0.81-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 40kB 4.3MB/s \n","\u001b[?25hCollecting sphinx-rtd-theme\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/24/2475e8f83519b54b2148d4a56eb1111f9cec630d088c3ffc214492c12107/sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1MB)\n","\u001b[K     |████████████████████████████████| 9.2MB 7.6MB/s \n","\u001b[?25hCollecting sphinx-argparse\n","  Downloading https://files.pythonhosted.org/packages/06/2b/dfad6a1831c3aeeae25d8d3d417224684befbf45e10c7f2141631616a6ed/sphinx-argparse-0.2.5.tar.gz\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n","Collecting morfessor\n","  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n","Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from sphinx-rtd-theme-\u003eindic-nlp-library) (1.8.5)\n","Collecting docutils\u003c0.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/44/8a15e45ffa96e6cf82956dd8d7af9e666357e16b0d93b253903475ee947f/docutils-0.16-py2.py3-none-any.whl (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 23.5MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eindic-nlp-library) (2.8.1)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003eindic-nlp-library) (2018.9)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (1.15.0)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (1.2.0)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (1.2.4)\n","Requirement already satisfied: requests\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.23.0)\n","Requirement already satisfied: babel!=2.0,\u003e=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.9.1)\n","Requirement already satisfied: alabaster\u003c0.8,\u003e=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (0.7.12)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (57.0.0)\n","Requirement already satisfied: Jinja2\u003e=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.11.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (20.9)\n","Requirement already satisfied: snowballstemmer\u003e=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.1.0)\n","Requirement already satisfied: Pygments\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.6.1)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (1.1.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.0.0-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2021.5.30)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.0.0-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.0.0-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.0.0-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.10)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2\u003e=2.3-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.0.1)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003esphinx-\u003esphinx-rtd-theme-\u003eindic-nlp-library) (2.4.7)\n","Building wheels for collected packages: sphinx-argparse\n","  Building wheel for sphinx-argparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sphinx-argparse: filename=sphinx_argparse-0.2.5-cp37-none-any.whl size=11552 sha256=bf031f66068bb7a3babae8c4b601913644c3f5c75ba6d8cb0de0b75f15b65447\n","  Stored in directory: /root/.cache/pip/wheels/2a/18/1b/4990a1859da4edc77ab312bc2986c08d2733fb5713d06e44f5\n","Successfully built sphinx-argparse\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: docutils, sphinx-rtd-theme, sphinx-argparse, morfessor, indic-nlp-library\n","  Found existing installation: docutils 0.17.1\n","    Uninstalling docutils-0.17.1:\n","      Successfully uninstalled docutils-0.17.1\n","Successfully installed docutils-0.16 indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.2.5 sphinx-rtd-theme-0.5.2\n","Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/a9/b3cea4a97ffabd6639e71608814dbd08081e202e8ac9580250273c0541ff/torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4MB)\n","\u001b[K     |████████████████████████████████| 831.4MB 22kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","\u001b[31mERROR: torchvision 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp 2.5.0 has requirement torch\u003c1.9.0,\u003e=1.6.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp-models 2.5.0 has requirement torch\u003c1.9.0,\u003e=1.7.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.8.1\n","    Uninstalling torch-1.8.1:\n","      Successfully uninstalled torch-1.8.1\n","Successfully installed torch-1.9.0\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 7.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied, skipping upgrade: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied, skipping upgrade: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.0.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12-\u003etransformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.5.30)\n","Requirement already satisfied, skipping upgrade: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003etransformers) (3.4.1)\n","Requirement already satisfied, skipping upgrade: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers) (2.4.7)\n","\u001b[31mERROR: allennlp 2.5.0 has requirement torch\u003c1.9.0,\u003e=1.6.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp 2.5.0 has requirement transformers\u003c4.7,\u003e=4.1, but you'll have transformers 4.8.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp-models 2.5.0 has requirement torch\u003c1.9.0,\u003e=1.7.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n","Installing collected packages: huggingface-hub, transformers\n","  Found existing installation: huggingface-hub 0.0.13\n","    Uninstalling huggingface-hub-0.0.13:\n","      Successfully uninstalled huggingface-hub-0.0.13\n","  Found existing installation: transformers 4.6.1\n","    Uninstalling transformers-4.6.1:\n","      Successfully uninstalled transformers-4.6.1\n","Successfully installed huggingface-hub-0.0.12 transformers-4.8.2\n","Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n","\u001b[K     |████████████████████████████████| 92kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: transformers\u003c5.0.0,\u003e=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.8.2)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0)\n","Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.5)\n","Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n","Requirement already satisfied, skipping upgrade: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (20.9)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (3.13)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (4.6.0)\n","Requirement already satisfied, skipping upgrade: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (0.10.3)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (0.0.45)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.6.0-\u003esentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: pillow\u003e=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003esentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003esentence-transformers) (1.0.1)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk-\u003esentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2021.5.30)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (3.4.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (1.15.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=6ddc89a743b5c0dfaad7dd11c2ca7de9ab687c973645e642bed2cf181473c449\n","  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n","Successfully built sentence-transformers\n","Installing collected packages: sentence-transformers\n","Successfully installed sentence-transformers-2.0.0\n","Collecting google_trans_new\n","  Downloading https://files.pythonhosted.org/packages/f9/7b/9f136106dc5824dc98185c97991d3cd9b53e70a197154dd49f7b899128f6/google_trans_new-1.1.9-py3-none-any.whl\n","Installing collected packages: google-trans-new\n","Successfully installed google-trans-new-1.1.9\n","Collecting stanfordnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n","Requirement already satisfied: torch\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.41.1)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003estanfordnlp) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003estanfordnlp) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003estanfordnlp) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003estanfordnlp) (1.24.3)\n","Requirement already satisfied: six\u003e=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf-\u003estanfordnlp) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.0.0-\u003estanfordnlp) (3.7.4.3)\n","Installing collected packages: stanfordnlp\n","Successfully installed stanfordnlp-0.2.0\n","Using the default treebank \"hi_hdtb\" for language \"hi\".\n","Would you like to download the models for: hi_hdtb now? (Y/n)\n","\n","Default download directory: /root/stanfordnlp_resources\n","Hit enter to continue or type an alternate directory.\n","\n","Downloading models for: hi_hdtb\n","Download location: /root/stanfordnlp_resources/hi_hdtb_models.zip\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 208M/208M [00:36\u003c00:00, 5.72MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Download complete.  Models saved to: /root/stanfordnlp_resources/hi_hdtb_models.zip\n","Extracting models file for: hi_hdtb\n","Cleaning up...Done.\n","Use device: gpu\n","---\n","Loading: tokenize\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n","---\n","Loading: pos\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/hi_hdtb_models/hi_hdtb.pretrain.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n","Done loading processors!\n","---\n"]}],"source":["! pip install allennlp\n","! pip install allennlp-models\n","! pip install nltk==3.5\n","\n","import io\n","import numpy as np\n","# from google_trans_new import google_translator \n","from scipy.spatial.distance import cosine\n","import json \n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","from nltk import word_tokenize\n","from itertools import groupby \n","from spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI\n","\n","from tqdm import tqdm\n","import pandas as pd\n","\n","!pip install indic-nlp-library\n","import indicnlp\n","from indicnlp.tokenize import indic_tokenize \n","!pip install -U torch\n","import torch\n","!pip install -U transformers\n","!pip install -U sentence-transformers\n","import transformers\n","from sentence_transformers import SentenceTransformer,util\n","import numpy as np\n","!pip install google_trans_new\n","from google_trans_new import google_translator\n","!pip install stanfordnlp\n","import stanfordnlp \n","stanfordnlp.download('hi')\n","nlp = stanfordnlp.Pipeline(processors = \"tokenize,pos\",lang = 'hi')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6pRWUbPG_1n"},"outputs":[],"source":["from allennlp.predictors import Predictor\n","predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz\")\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LERJ60u9mL4a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7zQOoUxPpgS"},"outputs":[],"source":["# model = SentenceTransformer('stsb-xlm-r-multilingual')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","# model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n","model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n","# model = SentenceTransformer('msmarco-distilbert-base-v2')\n","# model = SentenceTransformer('LaBSE')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjJhZfJKRkhB"},"outputs":[],"source":["#@title\n","results = predictor.predict(sentence=\"position held Member of the 11th Lok Sabha, Rajya Sabha\")\n","named_entities = []\n","flag = 0\n","named = \"\"\n","for word, tag in zip(results[\"words\"], results[\"tags\"]):\n","    if tag != 'O':\n","      flag = 1\n","      named = named+ word + \" \"\n","    else:\n","      if flag:\n","        named_entities.append(named.strip(' '))\n","      named = \"\"\n","      flag = 0\n","    print(f\"{word}\\t{tag}\")\n","if flag:\n","  named_entities.append(named.strip(' '))\n","  named = \"\"\n","named_entities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLvNCxdoJ8vY"},"outputs":[],"source":["!pip install mtranslate\n","from mtranslate import translate\n","translate(\"कमला\",\"en\",\"auto\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8urRp92Zsc5"},"outputs":[],"source":["pos_dict = {\n","'CC': 'coordinating conjunction','CD': 'cardinal digit','DT': 'determiner',\n","'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n","'FW': 'foreign word','IN':  'preposition/subordinating conjunction','JJ': 'adjective \\'big\\'',\n","'JJR': 'adjective, comparative \\'bigger\\'','JJS': 'adjective, superlative \\'biggest\\'',\n","'LS': 'list marker 1)','MD': 'modal could, will','NN': 'noun, singular \\'desk\\'',\n","'NNS': 'noun plural \\'desks\\'','NNP': 'proper noun, singular \\'Harrison\\'',\n","'NNPS': 'proper noun, plural \\'Americans\\'','PDT': 'predeterminer \\'all the kids\\'',\n","'POS': 'possessive ending parent\\'s','PRP': 'personal pronoun I, he, she',\n","'PRP$': 'possessive pronoun my, his, hers','RB': 'adverb very, silently,',\n","'RBR': 'adverb, comparative better','RBS': 'adverb, superlative best',\n","'RP': 'particle give up','TO': 'to go \\'to\\' the store.','UH': 'interjection errrrrrrrm',\n","'VB': 'verb, base form take','VBD': 'verb, past tense took',\n","'VBG': 'verb, gerund/present participle taking','VBN': 'verb, past participle taken',\n","'VBP': 'verb, sing. present, non-3d take','VBZ': 'verb, 3rd person sing. present takes',\n","'WDT': 'wh-determiner which','WP': 'wh-pronoun who, what','WP$': 'possessive wh-pronoun whose',\n","'WRB': 'wh-abverb where, when','QF' : 'quantifier, bahut, thoda, kam (Hindi)','VM' : 'main verb',\n","'PSP' : 'postposition, common in indian langs','DEM' : 'demonstrative, common in indian langs'\n","}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WdyQaByZPqEw"},"outputs":[],"source":["#@title\n","sentence1 = \"country of citizenship Austria\"\n","sentence2 =  'ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है'\n","# sentence2 =  'ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है'\n","# sentence2 = \"एक भारतीय राजनीतिज्ञ था\"\n","words = [w for w in indic_tokenize.trivial_tokenize(sentence2, lang = 'hi') if w not in STOP_WORDS_HI]\n","print(words)\n","sentence2 = ' '.join(words)\n","# encode sentences to get their embeddings\n","embedding1 = model.encode(sentence1, convert_to_tensor=True)\n","embedding2 = model.encode(sentence2, convert_to_tensor=True)\n","# compute similarity scores of two embeddings\n","cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","print(\"Sentence 1:\", sentence1)\n","print(\"Sentence 2:\", sentence2)\n","print(\"Similarity score:\", cosine_scores.item())\n","\n","# {('Surjit Singh Barnala',\n","#    'country of citizenship',\n","#    'British India'),\n","#   ('Surjit Singh Barnala', 'country of citizenship', 'Dominion of India'),\n","#   ('Surjit Singh Barnala', 'country of citizenship', 'India'),\n","#   ('Surjit Singh Barnala', 'date of birth', '1925-10-21T00:00:00Z'),\n","#   ('Surjit Singh Barnala', 'date of death', '2017-01-14T00:00:00Z'),\n","#   ('Surjit Singh Barnala', 'occupation', 'politician')},\n","\n","#  'सुरजीत सिंह बरनाला (21 अक्टूबर 1925 – 14 जनवरी 2017) एक भारतीय राजनीतिज्ञ था': {('Surjit Singh Barnala',\n","#    'occupation',\n","#    'politician'),\n","#   ('Surjit Singh Barnala', 'position held', 'Chief Minister of Punjab'),\n","#   ('Surjit Singh Barnala', 'position held', 'Member of the 11th Lok Sabha'),\n","#   ('Surjit Singh Barnala', 'position held', 'Member of the 12th Lok Sabha'),\n","#   ('Surjit Singh Barnala',\n","#    'position held',\n","#    'Member of the Punjab Legislative Assembly'),\n","#   ('Surjit Singh Barnala', 'position held', 'governor of Andhra Pradesh'),\n","#   ('Surjit Singh Barnala', 'position held', 'governor of Uttarakhand')},"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJGI5D4KaH5C"},"outputs":[],"source":["for word in words:\n","  if word.isdigit():\n","    print(word)\n","  else:\n","    if len(word) \u003e 2:\n","      print(word , get_nn(word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5))"]},{"cell_type":"markdown","metadata":{"id":"MtnTpTPGGYP3"},"source":["### Loading data from json file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTxwQR9yGYP4"},"outputs":[],"source":["import json \n","# Opening JSON file \n","testA,testC,testP = open('/content/drive/MyDrive/test_data/testA.json',encoding = 'utf-8') ,open('/content/drive/MyDrive/test_data/testC2.json',encoding = 'utf-8'),open('/content/drive/MyDrive/test_data/testP.json',encoding = 'utf-8')\n","# testA,testC,testP = open('/content/drive/MyDrive/test_data/testA.json',encoding = 'utf-8') ,open('/content/drive/MyDrive/test_data/testC2.json',encoding = 'utf-8'),open('/content/drive/MyDrive/test_data/politicians.json',encoding = 'utf-8')\n","# testA,testC,testP = open('/content/drive/MyDrive/test_data/testA.json',encoding = 'utf-8') ,open('/content/drive/MyDrive/test_data/testC2.json',encoding = 'utf-8'),open('/content/drive/MyDrive/test_data/corrected_annotated_data/politicians_test(corrected).json',encoding = 'utf-8')\n","dataA,dataC,dataP = json.load(testA),json.load(testC),json.load(testP),\n","# returns JSON object as  \n","# a dictionary \n","# data = json.load(f) \n","# d = {}\n","# d = data\n","  \n","# Closing file \n","# f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUdVuYkfBdY-"},"outputs":[],"source":["for qid in dataC:\n","  t =  dataC[qid]\n","  for key in t:\n","    if key == 'triples':\n","      triplelist = t[key]\n","      for l in triplelist:\n","        if len(l) == 1:\n","          triplelist.remove(l)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voc83SuKGYP6"},"outputs":[],"source":["### Getting a_c_p for getting all the triples\n","# Opening JSON file \n","f = open('/content/drive/MyDrive/a_c_p.json',) \n","data = json.load(f) \n","acp = {}\n","acp = data\n","f.close() "]},{"cell_type":"markdown","metadata":{"id":"6CuI3DLzN2iS"},"source":["# Word Overlap functions"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":684,"status":"ok","timestamp":1626254983691,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"SzIDlOmwOD-_"},"outputs":[],"source":["\n","def load_vec(emb_path, nmax=50000):\n","    vectors = []\n","    word2id = {}\n","    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n","        next(f)\n","        for i, line in enumerate(f):\n","            word, vect = line.rstrip().split(' ', 1)\n","            vect = np.fromstring(vect, sep=' ')\n","            assert word not in word2id, 'word found twice'\n","            vectors.append(vect)\n","            word2id[word] = len(word2id)\n","            if len(word2id) == nmax:\n","                break\n","    id2word = {v: k for k, v in word2id.items()}\n","    embeddings = np.vstack(vectors)\n","    return embeddings, id2word, word2id"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8803,"status":"ok","timestamp":1626254992489,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"HXlecBzGOFsl"},"outputs":[],"source":["dir = '/content/drive/My Drive/Algo Name detection implementation/Text/'\n","src_path = '/content/drive/My Drive/wiki.hi.align.vec'\n","tgt_path = '/content/drive/My Drive/wiki.en.align.vec'\n","nmax = 50000  # maximum number of word embeddings to load\n","\n","src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n","tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"]},{"cell_type":"markdown","metadata":{"id":"1RSWeZ2aOOPw"},"source":["Get nearest Neighbours"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1626254992490,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"nTNL47EBOIP9"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n","    # print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n","    word2id = {v: k for k, v in src_id2word.items()}\n","    targetwordlist = []                               # List of target words for the source word \n","    if word in word2id:                               #Check if word is in vocab\n","      word_emb = src_emb[word2id[word]]\n","      scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n","      k_best = scores.argsort()[-K:][::-1]\n","      for i, idx in enumerate(k_best):\n","          # print((scores[idx], tgt_id2word[idx]))      #To give both distance and word\n","          if tgt_id2word[idx] not in stop_words:\n","            targetwordlist.append(tgt_id2word[idx].lower())\n","      return targetwordlist\n","    else:\n","      # translator = google_translator()\n","      # translate_text = translator.translate(word,lang_tgt='en') \n","      translate_text = translate(word,\"en\",\"auto\")\n","      transw = translate_text\n","      if transw not in stop_words:\n","      # print(word,\" - Translated - \",transw)\n","        return tuple([transw.lower()])\n","      "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1626254992491,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"Rql4YVS-35dI","outputId":"53184b9e-0945-43f3-817b-dca33e5bd766"},"outputs":[{"data":{"text/plain":["('kamla',)"]},"execution_count":15,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["get_nn('Kamla', src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"]},{"cell_type":"markdown","metadata":{"id":"vVmHUNcYN8_3"},"source":["# Sentence to Triple Matching"]},{"cell_type":"markdown","metadata":{"id":"FkUpq2A4GYQB"},"source":["### Obtaining sentence and triple embeddings\n","We obtain the sentence embeddings by taking each sentence, obtaining word embeddings for each word in the sentence, and averaging the word embeddings. For triple embeddings , we average the embeddings for each word in the triple."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":450,"status":"ok","timestamp":1625947306875,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"mK9bgHJJGYQC","outputId":"4b48c1eb-bfdb-4b96-b3d9-8f20f7164524"},"outputs":[{"data":{"text/plain":["(50, 100, 100)"]},"execution_count":7,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Matching Triples with sentences\n","actors_test, cricketers_test, politicians_test = dataA, dataC, dataP\n","actors, cricketers, politicians = actors_test, cricketers_test, politicians_test\n","\n","# Creating the dictionary for the test data where key = sentence and value = list of matching triples\n","actors_test_dict, cricketers_test_dict, politicians_test_dict = {}, {}, {}\n","for l in actors_test:\n","    for k, v in actors_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    actors_test_dict[sentence] = set(t)\n","\n","for l in cricketers_test:\n","    for k, v in cricketers_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    cricketers_test_dict[sentence] = set(t)\n","\n","for l in politicians_test:\n","    for k, v in politicians_test[l].items():\n","        if k == 'sentence':\n","            sentence = v\n","        if k == 'triples':\n","            triple_list = v\n","    t = [(e['subject'], e['predicate'], e['object']) for e in triple_list]\n","    politicians_test_dict[sentence] = set(t)\n","# for l in politicians_test:\n","#   entries = politicians_test[l]\n","#   subject = entries['personLabel']\n","#   triple_list = entries['triples']\n","  \n","    \n","len(actors_test_dict),len(cricketers_test_dict),len(politicians_test_dict)"]},{"cell_type":"markdown","metadata":{"id":"IJAOisIkJ-E-"},"source":["Keeping only the gold standard Sentences in politcians"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83kuFZ2WKCzX"},"outputs":[],"source":["gold_standard_sentences = ['रफी उद-दर्जत(३० नवम्बर १६९९-१७१९), रफ़ी-उस-शहान का कनिष्ठ पुत्र (अज़ीम उश शान का भाई) दसवां मुगल सम्राट था',\n"," 'अबेदिन नेप्राविश्ता एक अल्बानियाई राजनीतिज्ञ और दो बार (१९३३-१९३५ और १९३७-१९३९) तिराना के भूतपूर्व महापौर थे',\n"," 'अशोक चव्हाण भारत की सोलहवीं लोक सभा के सांसद हैं',\n"," 'एरुडोल्फ आइंस्टीन की बेटी, एल्सा का जन्म हेकिंगेन में जनवरी 1876 को हुआ था उन्हें 2 बहने थी पाउला (सी',\n"," 'मिना अहादी (फ़ारसी: مینا احدی‎) (जन्म 1956) एक ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है',\n"," 'याजुद्दिन अहमद (बांग्ला: য়াজউদ্দিন আহম্মেদ) (१ फरवरी, 1931) के वर्तमान राष्ट्रपति बांग्लादेश की है और कार्यालय में 2002 के बाद से पैदा की गई है']\n","\n","temp = {}\n","for key_sentence in politicians_test_dict:\n","  if key_sentence in gold_standard_sentences:\n","    temp[key_sentence] = politicians_test_dict[key_sentence]\n","politicians_test_dict = temp"]},{"cell_type":"markdown","metadata":{"id":"VIfca8mnNbYF"},"source":["Cleaning the annotated triples --  We need to prune out certain triples in politicians -- like date of birth, data of death in predicate, British India and Dominion of India in objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bK1OXiTjNVcM"},"outputs":[],"source":["#@title\n","temp = {}\n","for sentence in politicians_test_dict:\n","  triplelist = politicians_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","politicians_test_dict = temp\n","\n","temp = {}\n","for sentence in actors_test_dict:\n","  triplelist = actors_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or 'name' in triple[1] or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","actors_test_dict = temp\n","\n","temp = {}\n","for sentence in cricketers_test_dict:\n","  triplelist = cricketers_test_dict[sentence]\n","  new_triple_list = set()\n","  for triple in triplelist:\n","    if triple[1] == 'date of birth' or triple[1] == 'date of death' or triple[1] == 'sex or gender' or triple[2] == 'British India' or triple[2] == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.add(triple)\n","    temp[sentence] = new_triple_list\n","cricketers_test_dict = temp"]},{"cell_type":"markdown","metadata":{"id":"ivQ_xQkBPGyt"},"source":["Loading ACP.json for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHka8MKOYDmx"},"outputs":[],"source":["f = open('/content/drive/MyDrive/test_data/politicians.json',encoding = 'utf-8')\n","acp_p = json.load(f)\n","f = open('/content/drive/MyDrive/test_data/actors.json',encoding = 'utf-8')\n","acp_a = json.load(f)\n","f = open('/content/drive/MyDrive/test_data/cricketers.json',encoding = 'utf-8')\n","acp_c = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DklRia_dvMub"},"outputs":[],"source":["#@title\n","# Run this cell to randomly sample, else comment it out to run evaluate. This is for creation of train alignment.\n","# from random import sample\n","# keys = sample(list(acp_p.keys()), 100)\n","# acp_p_random_sampled = {}\n","# for key in keys:\n","#   acp_p_random_sampled[key] = acp_p[key]\n","\n","# keys = sample(list(acp_a.keys()), 100)\n","# acp_a_random_sampled = {}\n","# for key in keys:\n","#   acp_a_random_sampled[key] = acp_a[key]\n","\n","# keys = sample(list(acp_c.keys()), 100)\n","# acp_c_random_sampled = {}\n","# for key in keys:\n","#   acp_c_random_sampled[key] = acp_c[key]\n","# acp_p, acp_a, acp_c = acp_p_random_sampled, acp_a_random_sampled, acp_c_random_sampled\n","\n","# politicians_sent= [acp_p[key]['sentences']  for key in acp_p]\n","\n","# # politicians_sent = []\n","# # for key in acp_p:\n","# #   print(acp_p[key]['sentences'])\n","# #   # entries = acp_p[key]\n","# #   # print(entries)\n","# #   politicians_sent.append(acp_p[key]['sentences'])\n","# politicians_trip = [acp_p[key]['triples']  for key in acp_p]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-1c1Sh9FGYQE"},"outputs":[],"source":["#@title\n","# #@title\n","# # We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n","# actors_trip, cricketers_trip, politician_trip = [], [], []\n","# actors_sent, cricketers_sent, politician_sent = [], [], []\n","\n","# # Putting actors,cricketers and politicians from a_c_p\n","# # act, cric, pol = acp['a'], acp['c'], acp['p']      # Original acp.json\n","\n","# # Now, we are using better, cleaned , pruned version of acp json, ( cleaned by Shivprasad)\n","# act = acp_a\n","# cric = acp_c\n","# pol = acp_p\n","# ############### Actors #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in actors_test:\n","#     eid = actors_test[e]['entity_id']\n","#     for ele in act:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = act[ele]['triples']\n","#             subject = act[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             actors_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in actors_test:\n","#         if eid == actors_test[e]['entity_id']:\n","#             sentence = actors_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     actors_sent.append(sentence_list)\n","\n","\n","# ############### Cricketers #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in cricketers_test:\n","#     eid = cricketers_test[e]['entity_id']\n","#     for ele in cric:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = cric[ele]['triples']\n","#             subject = cric[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             cricketers_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in cricketers_test:\n","#         if eid == cricketers_test[e]['entity_id']:\n","#             sentence = cricketers_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     cricketers_sent.append(sentence_list)\n","\n","\n","# ############### Politicians #######################\n","\n","# # Iterating over test annotated data and keeping relevant triples only\n","# entity_tracking = []\n","# for e in politicians_test:\n","#     eid = politicians_test[e]['entity_id']\n","#     for ele in pol:\n","#         if eid == ele and eid not in entity_tracking:\n","#             entity_tracking.append(eid)\n","#             # Getting triples for the matching entity id\n","#             triples = pol[ele]['triples']\n","#             subject = pol[ele]['personLabel']\n","#             triplist = []\n","#             for trip in triples:\n","#                 predicate = trip['propertyLabel']\n","#                 obj = trip['objectLabel']\n","#                 trip_tuple = (subject, predicate, obj)\n","#                 triplist.append(trip_tuple)\n","#             politician_trip.append(triplist)\n","\n","# # Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","# for eid in entity_tracking:\n","#     sentence_list = []\n","#     for e in politicians_test:\n","#         if eid == politicians_test[e]['entity_id']:\n","#             sentence = politicians_test[e]['sentence']\n","#             sentence_list.append(sentence)\n","#     politician_sent.append(sentence_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiPxAFIbQZag"},"outputs":[],"source":["#@title\n","# We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n","actors_trip, cricketers_trip, politician_trip = [], [], []\n","actors_sent, cricketers_sent, politician_sent = [], [], []\n","\n","# Putting actors,cricketers and politicians from a_c_p\n","# act, cric, pol = acp['a'], acp['c'], acp['p']      # Original acp.json\n","\n","# Now, we are using better, cleaned , pruned version of acp json, ( cleaned by Shivprasad)\n","act = acp_a\n","cric = acp_c\n","pol = acp_p\n","############### Actors #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in actors_test:\n","    eid = actors_test[e]['entity_id']\n","    for ele in act:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = act[ele]['triples']\n","            subject = act[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip[1]\n","                obj = trip[2]\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            actors_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in actors_test:\n","        if eid == actors_test[e]['entity_id']:\n","            sentence = actors_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    actors_sent.append(sentence_list)\n","\n","\n","############### Cricketers #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in cricketers_test:\n","    eid = cricketers_test[e]['entity_id']\n","    for ele in cric:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = cric[ele]['triples']\n","            subject = cric[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            cricketers_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in cricketers_test:\n","        if eid == cricketers_test[e]['entity_id']:\n","            sentence = cricketers_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    cricketers_sent.append(sentence_list)\n","\n","\n","############### Politicians #######################\n","\n","# Iterating over test annotated data and keeping relevant triples only\n","entity_tracking = []\n","for e in politicians_test:\n","    eid = politicians_test[e]['entity_id']\n","    for ele in pol:\n","        if eid == ele and eid not in entity_tracking:\n","            entity_tracking.append(eid)\n","            # Getting triples for the matching entity id\n","            triples = pol[ele]['triples']\n","            subject = pol[ele]['personLabel']\n","            triplist = []\n","            for trip in triples:\n","                predicate = trip['propertyLabel']\n","                obj = trip['objectLabel']\n","                trip_tuple = (subject, predicate, obj)\n","                triplist.append(trip_tuple)\n","            politician_trip.append(triplist)\n","\n","# Iterating over test annotated data and grouping annotated sentences together by entity id\n","\n","for eid in entity_tracking:\n","    sentence_list = []\n","    for e in politicians_test:\n","        if eid == politicians_test[e]['entity_id']:\n","            sentence = politicians_test[e]['sentence']\n","            sentence_list.append(sentence)\n","    politician_sent.append(sentence_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1625947339332,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"s-TnH9IuGYQN","outputId":"f941aa55-3393-4418-f47b-a19f8b9d7407"},"outputs":[{"data":{"text/plain":["(50, 100, 99)"]},"execution_count":11,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(actors_sent), len(cricketers_sent),len(politician_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1625947340916,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"JjiX1jl3GYQU","outputId":"a9c23746-fa22-4d23-9410-a977cdaad076"},"outputs":[{"data":{"text/plain":["(50, 100, 99)"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(actors_trip), len(cricketers_trip),len(politician_trip)"]},{"cell_type":"markdown","metadata":{"id":"QA7DSxyRMDGD"},"source":["The train sample space has been created above. Now, we only keep the gold standard sentences in politicians "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1623928320384,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"-iPgs0YTMMkV","outputId":"ff916146-bb10-4912-b4b4-7b32b6f219aa"},"outputs":[{"data":{"text/plain":["(6, 6)"]},"execution_count":22,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["pol_sent,pol_trip = [],[]\n","for s,t in zip(politician_sent,politician_trip):\n","  if s[0] in gold_standard_sentences:\n","    pol_sent.append(s)\n","    pol_trip.append(t)\n","len(pol_sent),len(pol_trip)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1urG_vumGYQW"},"outputs":[],"source":["#@title\n","# def matches(sentences,triples):       # Input is a sentence list and triple list for one article\n","#     matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","#     for sent in sentences:            # For each sent in sentences\n","#         ent_matchlist = []\n","#         # words = [w for w in indic_tokenize.trivial_tokenize(sent, lang = 'hi') if w not in STOP_WORDS_HI]\n","#         # sent_no_stop = ' '.join(words)\n","#         sent_embed = model.encode(sent, convert_to_tensor=True)\n","#         for ent in triples:\n","#             pred = ent[1]\n","#             obj = ent[2]\n","#             # print(pred)\n","#             ent_embed = model.encode(pred + \" \" + obj, convert_to_tensor=True)\n","#             cosine_score = util.pytorch_cos_sim(sent_embed, ent_embed)\n","#             similarity = cosine_score.item()\n","            \n","#             if similarity \u003e 0.7:\n","#               # For evaluation of precision and recall, keep the below 3 lines commented out. They are to append score to matching triples\n","#                 # ent = list(ent)\n","#                 # ent.append(similarity)\n","#                 # ent = tuple(ent)\n","#                 ent_matchlist.append(ent)\n","#         if len(ent_matchlist)\u003e0:    \n","#             matches_dict[sent] = set(ent_matchlist)\n","#     return matches_dict\n","\n","# def matches(sentences,triples):       # Input is a sentence list and triple list for one article\n","#     matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","#     for sent in sentences:            # For each sent in sentences\n","#         ent_matchlist = []\n","#         words = [w for w in indic_tokenize.trivial_tokenize(sent, lang = 'hi') if w not in STOP_WORDS_HI]\n","#         sent_no_stop = ' '.join(words)\n","#         sent_embed = model.encode(sent_no_stop, convert_to_tensor=True)\n","#         for ent in triples:\n","#             subj = ent[0]\n","#             pred = ent[1]\n","#             obj = ent[2]\n","#             # print(pred)\n","#             ent_embed_1 = model.encode(subj + \" \" + pred + \" \" + obj, convert_to_tensor=True)\n","#             ent_embed_2 = model.encode(pred + \" \" + obj, convert_to_tensor=True)\n","#             cosine_score_1 = util.pytorch_cos_sim(sent_embed, ent_embed_1)\n","#             cosine_score_2 = util.pytorch_cos_sim(sent_embed, ent_embed_2)\n","#             similarity_1 = cosine_score_1.item()\n","#             similarity_2 = cosine_score_2.item()\n","#             similarity = max(similarity_1, similarity_2)\n","#             if similarity \u003e 0.45:\n","#                 print(subj + \" \" + pred + \" \" + obj + \" == \", similarity_1 , \", \", similarity_2)\n","#               # For evaluation of precision and recall, keep the below 3 lines commented out. They are to append score to matching triples\n","#                 # ent = list(ent)\n","#                 # ent.append(similarity)\n","#                 # ent = tuple(ent)\n","#                 ent_matchlist.append(ent)\n","#         if len(ent_matchlist)\u003e0:    \n","#             matches_dict[sent] = set(ent_matchlist)\n","#     return matches_dict"]},{"cell_type":"markdown","metadata":{"id":"HCsttlT7zLk_"},"source":["# Using New Test data"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":687,"status":"ok","timestamp":1626255007623,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"_4n9aKSBDaP6"},"outputs":[],"source":["def clean_triples(trip):\n","  trip2 = []\n","  for tlist in trip:\n","    triple_list = [t for t in tlist if t[1] not in remove_set \n","                  and t[2] not in remove_set and 'name' not in t[1]]\n","    trip2.append(triple_list)\n","  return trip2"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"elapsed":589,"status":"ok","timestamp":1626255011248,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"Sq_aujInzOxC","outputId":"1be07679-25f3-4d73-bc28-1d9240cf0056"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003elist of sentences\u003c/th\u003e\n","      \u003cth\u003elist of triples\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e[हेमलता तलेसरा ( जन्म 1 जुलाई 1944 ) एक भारतीय...\u003c/td\u003e\n","      \u003ctd\u003e[[Hemlata Talesra, date of birth, 01 July 1944...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e[वह एक लेखक , शोधकर्ता , शिक्षक हैं और भारत और...\u003c/td\u003e\n","      \u003ctd\u003e[[Hemlata Talesra, date of birth, 01 July 1944...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e[वी . सीतारमैया कन्नड़ भाषा के विख्यात साहित्य...\u003c/td\u003e\n","      \u003ctd\u003e[[V. Seetharamaiah, date of birth, 02 October ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e[इनके द्वारा रचित एक कविता–संग्रह अरलु बरलु के...\u003c/td\u003e\n","      \u003ctd\u003e[[V. Seetharamaiah, date of birth, 02 October ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e[हेमलता तलेसरा ( जन्म 1 जुलाई 1944 ) एक भारतीय...\u003c/td\u003e\n","      \u003ctd\u003e[[Hemlata Talesra, date of birth, 01 July 1944...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["   Unnamed: 0  ...                                    list of triples\n","0           0  ...  [[Hemlata Talesra, date of birth, 01 July 1944...\n","1           1  ...  [[Hemlata Talesra, date of birth, 01 July 1944...\n","2           2  ...  [[V. Seetharamaiah, date of birth, 02 October ...\n","3           3  ...  [[V. Seetharamaiah, date of birth, 02 October ...\n","4           4  ...  [[Hemlata Talesra, date of birth, 01 July 1944...\n","\n","[5 rows x 3 columns]"]},"execution_count":17,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import pandas as pd\n","TRAIN_CSV = '/content/drive/MyDrive/train_data/processed_train_data/train_writers.csv'\n","df = pd.read_csv(TRAIN_CSV, converters = {'list of sentences':eval,'list of triples':eval})\n","df.head()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2003,"status":"ok","timestamp":1626255091367,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"VHBZTls3zpdT","outputId":"22f74f15-10ad-40b2-e815-ebc1714b9604"},"outputs":[{"data":{"text/plain":["(50, 50)"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["remove_set = {'date of birth','date of death','sex or gender','British India','Dominion of India'}\n","writers_sent = list(df['list of sentences'])\n","writers_trip = list(df['list of triples'])\n","\n","writers_trip = clean_triples(writers_trip)\n","len(writers_sent),len(writers_trip)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1626255487783,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"Xu7lnsky0hWT"},"outputs":[],"source":["import json\n","TEST_ANNOTATED_DATA_PATH = '/content/drive/MyDrive/test_data/processed_test_data/writers_test_dict.json'\n","with open(TEST_ANNOTATED_DATA_PATH,'r') as file:\n","  writers_test_dict = json.loads(file.read())\n","\n","# converting lists to tuples and cleaning them\n","writers_test_dict = {s:[tuple(t) for t in tlist if t[1] not in remove_set and t[2] not in remove_set and 'name' not in t[1]]\n","                     for s,tlist in writers_test_dict.items()}"]},{"cell_type":"markdown","metadata":{"id":"sFTBBXUzOvSx"},"source":["# Alignment"]},{"cell_type":"markdown","metadata":{"id":"UJtQqu6xRWGB"},"source":["KeyPhrase Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gwM4PgTD52Pg"},"outputs":[],"source":["#@title\n","gold_standard_sentences = ['रफी उद-दर्जत(३० नवम्बर १६९९-१७१९), रफ़ी-उस-शहान का कनिष्ठ पुत्र (अज़ीम उश शान का भाई) दसवां मुगल सम्राट था',\n"," 'अबेदिन नेप्राविश्ता एक अल्बानियाई राजनीतिज्ञ और दो बार (१९३३-१९३५ और १९३७-१९३९) तिराना के भूतपूर्व महापौर थे',\n"," 'अशोक चव्हाण भारत की सोलहवीं लोक सभा के सांसद हैं',\n"," 'एरुडोल्फ आइंस्टीन की बेटी, एल्सा का जन्म हेकिंगेन में जनवरी 1876 को हुआ था उन्हें 2 बहने थी पाउला (सी',\n"," 'मिना अहादी (फ़ारसी: مینا احدی‎) (जन्म 1956) एक ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है',\n"," 'याजुद्दिन अहमद (बांग्ला: য়াজউদ্দিন আহম্মেদ) (१ फरवरी, 1931) के वर्तमान राष्ट्रपति बांग्लादेश की है और कार्यालय में 2002 के बाद से पैदा की गई है']"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1626255119938,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"d_on7YWmRkkh","outputId":"7ab65196-0782-453e-b738-eedb39821238"},"outputs":[{"name":"stdout","output_type":"stream","text":["सी रंगराजन राज्यसभा के सांसद\n","प्रसिद्ध अर्थशास्त्री\n"]}],"source":["#extract parts of speech\n","def extract_pos(doc):\n","    pos = [(wrd.text,wrd.pos) for sent in doc.sentences for wrd in sent.words]\n","    return pos\n","hindi = nlp('सी रंगराजन राज्यसभा के सांसद एवं प्रसिद्ध अर्थशास्त्री हैं')\n","extract_pos(hindi)\n","from nltk import RegexpParser\n","pos_tags = extract_pos(hindi)\n","# print(pos_tags)\n","# patterns = \"NP: {\u003cJJ\u003e*\u003cNN|NNS|NNC|NNCS|NNP|NNPS|NNPC|NNPCS\u003e+}\"\n","patterns = \"NP:{(\u003cJJ\u003e* \u003cNN.*\u003e+ \u003cPSP\u003e)? \u003cJJ\u003e* \u003cNN.*\u003e+}\"        #PSP -\u003e PostPositions are common in Indic Languages\n","PChunker = RegexpParser(patterns)\n","pos_tags = extract_pos(hindi)\n","output = PChunker.parse(pos_tags)\n","for subtree in output.subtrees(filter=lambda t: t.label() == 'NP'):\n","    print(' '.join([x[0] for x in subtree]))"]},{"cell_type":"markdown","metadata":{"id":"BmPPpuvTrPqz"},"source":["Finding Top K matches"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IDEGQEzPrSlU"},"outputs":[],"source":["#@title\n","from nltk.util import ngrams\n","sentence1 = \"country of citizenship Austria\"\n","sentence2 =     'ललित नारायण मिश्र 1923 से 1975 तक भारत के रेलमंत्री थे'\n","\n","# sentence2 =  'ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है'\n","hindi = nlp(sentence2)\n","extract_pos(hindi)\n","from nltk import RegexpParser\n","pos_tags = extract_pos(hindi)\n","patterns = \"NP: {\u003cJJ\u003e*\u003cNN|NNS|NNC|NNCS|NNP|NNPS|NNPC|NNPCS\u003e+}\"\n","# patterns = \"NP:{(\u003cJJ\u003e* \u003cNN.*\u003e+ \u003cPSP\u003e)? \u003cJJ\u003e* \u003cNN.*\u003e+}\" \n","PChunker = RegexpParser(patterns)\n","pos_tags = extract_pos(hindi)\n","output = PChunker.parse(pos_tags)\n","keyphrases = []\n","for subtree in output.subtrees(filter=lambda t: t.label() == 'NP'):\n","    keyphrases.append((' '.join([x[0] for x in subtree])))\n","\n","triple_list =  [('Lalit Narayan Mishra',\n","   'relative',\n","   'Nitish Mishra'),\n","  ('Lalit Narayan Mishra', 'child', 'Vijay Kumar Mishra'),\n","  ('Lalit Narayan Mishra', 'sibling', 'Jagannath Mishra'),\n","  ('Lalit Narayan Mishra', 'country of citizenship', 'India'),\n","  ('Lalit Narayan Mishra', 'occupation', 'politician')]\n","# for s,t in zip(pol_sent,pol_trip):\n","#   if s[0] == sentence2:\n","#     triple_list = t\n","matches = []\n","print(keyphrases)\n","for kp in keyphrases:\n","  words = [w for w in indic_tokenize.trivial_tokenize(kp, lang = 'hi') if w not in STOP_WORDS_HI]\n","  k = len(words)\n","  bigrams = ngrams(words,2)\n","  trigrams = ngrams(words,3)\n","  if len(words) == 1:\n","    bigrams,trigrams = [(words)],[(words)]\n","  matches_for_a_keyphrase = []\n","  for tup in bigrams:\n","    sbi = ' '.join(list(tup))\n","    for s,p,o in triple_list:\n","      embedding1 = model.encode(sbi, convert_to_tensor=True)\n","      embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","      cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","      if cosine_scores.item() \u003e 0.45:\n","        matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),sbi])\n","        matches_for_a_keyphrase.sort(reverse=True)\n","\n","  for tup in trigrams:\n","    sbi = ' '.join(list(tup))\n","    print(sbi)\n","    for s,p,o in triple_list:\n","      embedding1 = model.encode(sbi, convert_to_tensor=True)\n","      embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","      cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","      if cosine_scores.item() \u003e 0.45:\n","        matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),sbi])      \n","  matches_for_a_keyphrase.sort(reverse = True)\n","  # triple_matches = []\n","  # Now, we have to keep only (s,p,o) from matches_for_a_keyphrase in triple_matches\n","  # for _,triple,_ in matches_for_a_keyphrase:\n","    # triple_matches.append(triple) \n","  matches.extend(matches_for_a_keyphrase[:k])    # Selecting top k triples, where k is the number of words in a keyphrase. \n","  # matches.extend(list(set(triple_matches))[:k+1])\n","matches.sort(reverse = True)\n","matches   \n","# words = [w for w in indic_tokenize.trivial_tokenize(sentence2, lang = 'hi') if w not in STOP_WORDS_HI]\n","# print(words)\n","# sentence2 = ' '.join(words)\n","# # encode sentences to get their embeddings\n","# embedding1 = model.encode(sentence1, convert_to_tensor=True)\n","# embedding2 = model.encode(sentence2, convert_to_tensor=True)\n","# # compute similarity scores of two embeddings\n","# cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","# print(\"Sentence 1:\", sentence1)\n","# print(\"Sentence 2:\", sentence2)\n","# print(\"Similarity score:\", cosine_scores.item())"]},{"cell_type":"markdown","metadata":{"id":"taZDI66wj75T"},"source":["Note : 1.We must remove anything with the word \"name\" in predicate as pre-processing part.\n","2. Remove sex or gender"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1626255124871,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"LOJ36KfGOZiV"},"outputs":[],"source":["#@title\n","def matches(sentences,triples):       # Input is a sentence list and triple list for one article\n","    matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","    for sent in sentences:            # For each sent in sentences\n","        ent_matchlist = []\n","        hindiwords = [w for w in indic_tokenize.trivial_tokenize(sent, lang = 'hi') if w not in STOP_WORDS_HI]\n","        sent_no_stop = ' '.join(hindiwords)  # Making the snetence without stop words\n","        sent_embed = model.encode(sent_no_stop, convert_to_tensor=True)\n","        bucket = []\n","        # Creating word overlap bucket\n","        for word in hindiwords:\n","                if word.isdigit():\n","                  continue\n","                else:\n","                  if len(word) \u003e 2:\n","                    wordlist = get_nn(word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n","                    bucket.extend(wordlist)\n","\n","        for ent in triples:\n","            subj = ent[0]\n","            pred = ent[1]\n","            obj = ent[2]\n","            # print(pred)\n","\n","            # Finding named entities from triples\n","            results = predictor.predict(sentence=pred + \" \" + obj)\n","            named_entities = [word for word, tag in zip(results[\"words\"], results[\"tags\"]) if tag!='O']\n","            # named_entities = []\n","            # prev = \"\"\n","            # for word, tag in zip(results[\"words\"], results[\"tags\"]):\n","            #   if tag !='O':\n","            #     prev = prev + word + \" \"\n","            #   else:\n","            #     if prev !='':\n","            #       named_entities.append(prev.strip(' '))\n","            #     prev = \"\"\n","            # if prev!='':\n","            #   named_entities.append(prev.strip(' '))  \n","            flag = 0\n","            if named_entities:\n","              # If named entities exist in the triple\n","              named_entities = set([ele.lower() for ele in named_entities])\n","\n","              if len(set(bucket).intersection(named_entities)) / len(named_entities) \u003e= 0.75:\n","                flag = 1\n","                # print(sent,named_entities)\n","            if named_entities and flag == 0:\n","              continue\n","            # ent_embed_1 = model.encode(subj + \" \" + pred + \" \" + obj, convert_to_tensor=True)\n","            ent_embed_2 = model.encode(\"He/She is \"+ pred + \" \" + obj, convert_to_tensor=True)\n","            # cosine_score_1 = util.pytorch_cos_sim(sent_embed, ent_embed_1)\n","            cosine_score_2 = util.pytorch_cos_sim(sent_embed, ent_embed_2)\n","            # similarity_1 = cosine_score_1.item()\n","            similarity_2 = cosine_score_2.item()\n","            # similarity = max(similarity_1, similarity_2)\n","            similarity = similarity_2\n","            if similarity \u003e 0.19:\n","                # print(subj + \" \" + pred + \" \" + obj + \" == \", similarity_1 , \", \", similarity_2)\n","              # For evaluation of precision and recall, keep the below 3 lines commented out. They are to append score to matching triples\n","                # ent = list(ent)\n","                # ent.append(similarity)\n","                # ent = tuple(ent)\n","                ent_matchlist.append(ent)\n","        if len(ent_matchlist)\u003e0:  \n","            ent_matchlist = [tuple(t) for t in ent_matchlist]  \n","            matches_dict[sent] = set(ent_matchlist)\n","    return matches_dict"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":668,"status":"ok","timestamp":1626255193637,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"vw37ylbAuTDu"},"outputs":[],"source":["#@title\n","from nltk.util import ngrams\n","from nltk import RegexpParser\n","\n","def preprocess(triple_list):\n","  new_triple_list = []\n","  for s,p,o in triple_list:\n","    if p == 'date of birth' or p == 'date of death' or 'name' in p or p == 'sex or gender' or o == 'British India' or o == 'Dominion of India':\n","      continue\n","    else:\n","      new_triple_list.append((s,p,o))\n","  return new_triple_list\n","\n","def matches_kp(sentences,triples):       # Matches based on keyphrase\n","    matches_dict = {}                 # Dictionary to store the sentences with the matching triples. key = sent, value = triple\n","    for sent in sentences:\n","      sentence2 =  sent\n","      hindi = nlp(sentence2)\n","      extract_pos(hindi)\n","      pos_tags = extract_pos(hindi)\n","      patterns = \"NP: {\u003cJJ\u003e*\u003cNN|NNS|NNC|NNCS|NNP|NNPS|NNPC|NNPCS\u003e+}\"\n","      # patterns = \"NP:{(\u003cJJ\u003e* \u003cNN.*\u003e+ \u003cPSP\u003e)? \u003cJJ\u003e* \u003cNN.*\u003e+}\"\n","      PChunker = RegexpParser(patterns)\n","      pos_tags = extract_pos(hindi)\n","      output = PChunker.parse(pos_tags)\n","      keyphrases = []\n","      for subtree in output.subtrees(filter=lambda t: t.label() == 'NP'):\n","          keyphrases.append((' '.join([x[0] for x in subtree])))\n","      triple_list = triples\n","      triple_list = preprocess(triple_list)\n","      matches = []\n","      # keyphrases = ['ईरानी ऑस्ट्रियाई राजनीतिक कार्यकर्ता है']\n","      for kp in keyphrases:\n","        words = [w for w in indic_tokenize.trivial_tokenize(kp, lang = 'hi') if w not in STOP_WORDS_HI]\n","        k = len(words)\n","        bigrams = ngrams(words,2)\n","        trigrams = ngrams(words,3)\n","        if len(words) == 1:\n","          bigrams,trigrams = [(words)],[(words)]\n","        matches_for_a_keyphrase = []\n","        for tup in bigrams:\n","          sbi = ' '.join(list(tup))\n","          for s,p,o in triple_list:\n","            embedding1 = model.encode(sbi, convert_to_tensor=True)\n","            embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","            cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","            if cosine_scores.item() \u003e 0.45:\n","              matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),kp])\n","        for tup in trigrams:\n","          sbi = ' '.join(list(tup))\n","          for s,p,o in triple_list:\n","            embedding1 = model.encode(sbi, convert_to_tensor=True)\n","            embedding2 = model.encode(p+\" \"+o, convert_to_tensor=True)\n","            cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n","            if cosine_scores.item() \u003e 0.45:\n","              matches_for_a_keyphrase.append([cosine_scores.item(),(s,p,o),kp])\n","        \n","        matches_for_a_keyphrase.sort(reverse = True)\n","        triple_matches = []\n","        # Now, we have to keep only (s,p,o) from matches_for_a_keyphrase in triple_matches\n","        for _,triple,_ in matches_for_a_keyphrase:\n","          triple_matches.append(triple) \n","        matches.extend(triple_matches[:k+1])\n","        \n","      matches.sort(reverse = True)\n","      matches_dict[sent] = list(set(matches))   \n","    return matches_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362404,"status":"ok","timestamp":1625948487751,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"xQ0oYdzhGYQX","outputId":"ef0ef979-f193-497b-a740-a65eea3fee41"},"outputs":[{"name":"stderr","output_type":"stream","text":["99it [06:02,  3.66s/it]\n"]}],"source":["#@title\n","# matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(politician_sent, politician_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_pol[k] = v\n","\n","\n","# pol_matches_sent = [list(matches_pol.keys())]\n","# pol_matches_trips = list(matches_pol.values())\n","# ner_matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(politician_sent, politician_trip)):\n","#     for k, v in matches(sent_list, triple_list).items():\n","#         ner_matches_pol[k] = v\n","\n","# matches_act = {}\n","# for sent_list, triple_list in tqdm(zip(actors_sent, actors_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_act[k] = v\n","\n","# matches_cric = {}\n","# for sent_list, triple_list in tqdm(zip(cricketers_sent, cricketers_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_cric[k] = v\n","\n","# matches_pol = {}\n","# for sent_list, triple_list in tqdm(zip(pol_sent, pol_trip)):\n","#     for k, v in matches_kp(sent_list, triple_list).items():\n","#         matches_pol[k] = v"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133821,"status":"ok","timestamp":1626255920540,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"sU1mT69B7hnR"},"outputs":[{"name":"stderr","output_type":"stream","text":["25it [00:09,  2.64it/s]\n"]}],"source":["matches_writer = {}\n","for sent_list, triple_list in tqdm(zip(writers_sent, writers_trip)):\n","    for k, v in matches(sent_list, triple_list).items():\n","        matches_writer[k] = v \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tF4RmvqR79Gi"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"n5ZNCyInGYQY"},"source":["### Evaluation : Precision and Recall"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fefHrcbHCOVX"},"outputs":[],"source":["politicians_test_dict_corrected = json.load(open('/content/drive/MyDrive/test_data/corrected_annotated_data/politicians_test(corrected).json',encoding = 'utf-8'))\n","for sent in politicians_test_dict_corrected:\n","  triplist = politicians_test_dict_corrected[sent]\n","  tup = set()\n","  for s,p,o in triplist:\n","    tup.add((s,p,o))\n","  politicians_test_dict_corrected[sent] = tup"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":463,"status":"ok","timestamp":1626255926264,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"iZHJcNVyGYQY"},"outputs":[],"source":["def evaluate(test_dict, matches_dict):\n","    sum_prec = 0\n","    for key, val in matches_dict.items():\n","        tp, fp = 0, 0\n","        for k, v in test_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        if (tp+fp) != 0:\n","            prec = tp/(tp + fp)\n","        else:\n","            prec = 0\n","        sum_prec = prec + sum_prec\n","\n","    sum_rec = 0\n","    for k, v in test_dict.items():\n","        rec = 0\n","        tp, fp = 0, 0\n","        for key, val in matches_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        rec = tp/len(v)\n","        sum_rec = rec + sum_rec\n","\n","    avg_rec, avg_prec = sum_rec/len(test_dict), sum_prec/len(matches_dict)\n","    return avg_rec, avg_prec\n","\n","# avg_rec_act, avg_prec_act = evaluate(actors_test_dict, matches_act)\n","# avg_rec_cric, avg_prec_cric = evaluate(cricketers_test_dict, matches_cric)\n","# avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict_corrected, matches_pol)\n","# avg_rec_sing, avg_prec_sing = evaluate(singers_test_dict, matches_singer)\n","avg_rec_writ, avg_prec_writ = evaluate(writers_test_dict, matches_writer)"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":457,"status":"ok","timestamp":1626248903351,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"XULcBYdLB4AO"},"outputs":[],"source":["def global_evaluate(test_dict, matches_dict):\n","    sum_tp,sum_fp = 0,0\n","    total_count = 0\n","    for key, val in matches_dict.items():\n","        tp, fp = 0, 0\n","        for k, v in test_dict.items():\n","            # If sentence matches\n","            if k == key:\n","                for ent in v:\n","                    for trip in val:\n","                        if ent == trip:\n","                            tp = tp + 1\n","                for trip in val:\n","                    flag = 0\n","                    for ent in v:\n","                        if ent == trip:\n","                            flag = 1\n","                            break\n","                    if flag == 0:\n","                        fp = fp + 1\n","                break\n","        sum_tp += tp\n","        sum_fp += fp\n","\n","    \n","    for k, v in test_dict.items():\n","        total_count += len(v)\n","    \n","    prec = sum_tp/(sum_tp+sum_fp)\n","    rec = sum_tp/(total_count)\n","    return rec,prec\n","# avg_rec_act, avg_prec_act = global_evaluate(actors_test_dict, matches_act)\n","# avg_rec_cric, avg_prec_cric = global_evaluate(cricketers_test_dict, matches_cric)\n","# avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict_corrected, matches_pol)\n","avg_rec_sing, avg_prec_sing = global_evaluate(singers_test_dict, matches_singer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhT8DK_wHtzj"},"outputs":[],"source":["matches_pol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTVavgAExGwO"},"outputs":[],"source":["count  = 0\n","# Reason for Low recall : Missed Triples\n","for sent in writers_test_dict:\n","  if sent in matches_writer:\n","    triples_test = writers_test_dict[sent]\n","    triples_matches = matches_writer[sent]\n","    for t in triples_test:\n","      if t not in triples_matches:\n","          print(t)\n","          count +=1\n","print(count)\n","\n","\n","  # else:\n","  #   print(politicians_test_dict[sent])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXrw0D3jYU5C"},"outputs":[],"source":["for sent in writers_test_dict:\n","  if sent in matches_writer:\n","    triples_test = writers_test_dict[sent]\n","    triples_matches = matches_writer[sent]\n","    for t in triples_matches:\n","      if t not in triples_test:\n","        print(t)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1626255935949,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"8v5hAzGZFTVG","outputId":"bfe41be7-83f7-447b-9251-b34802d5b404"},"outputs":[{"data":{"text/plain":["(0.6170212765957447, 0.6760461760461761)"]},"execution_count":44,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["(avg_rec_writ,avg_prec_writ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":614,"status":"ok","timestamp":1625948582563,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"BX28FMolGYQZ","outputId":"39a0d9d1-12e7-4066-8625-2ee7de7a955e"},"outputs":[{"data":{"text/plain":["((0.75, 0.6641221374045801),\n"," (0.7734138972809668, 0.7804878048780488),\n"," (0.8095959595959595, 0.675974025974026))"]},"execution_count":30,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["(avg_rec_act,avg_prec_act), (avg_rec_cric, avg_prec_cric) , (avg_rec_pol, avg_prec_pol)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1625948587515,"user":{"displayName":"Swayatta Daw","photoUrl":"","userId":"08962232358580406420"},"user_tz":-330},"id":"MZnxKg-bGYQa","outputId":"7efd613e-34f1-4f95-ca4a-7b7393ccd742"},"outputs":[{"data":{"text/plain":["(0.7776699522923088, 0.7068613227522184)"]},"execution_count":31,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["AverageRecall = (avg_rec_act + avg_rec_cric + avg_rec_pol)/3\n","AveragePrecision = (avg_prec_act + avg_prec_cric + avg_prec_pol)/3\n","\n","AverageRecall, AveragePrecision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtUJcKUmglIr"},"outputs":[],"source":["politicians_test_dict"]},{"cell_type":"markdown","metadata":{"id":"EEC0ANMlGYQa"},"source":["### Output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XM6te62fHjPN"},"outputs":[],"source":["len(matches_act),len(matches_cric),len(matches_pol)"]},{"cell_type":"markdown","metadata":{"id":"L_3osDlHIOk6"},"source":["So, 7 sentences didn't find any matches at all.Let's see which ones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MW5RmuZpI1gU"},"outputs":[],"source":["for sent in politicians_test_dict:\n","  if sent not in matches_pol:\n","    print(sent,\" : \", politicians_test_dict[sent] )"]},{"cell_type":"markdown","metadata":{"id":"u_wNNBsXK7Hh"},"source":["Saving as json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2w2ym6zeNVA6"},"outputs":[],"source":["# for sent in actors_test_dict:\n","#   actors_test_dict[sent] = list(actors_test_dict[sent])\n","# for sent in cricketers_test_dict:\n","#   cricketers_test_dict[sent] = list(cricketers_test_dict[sent])\n","# for sent in politicians_test_dict:\n","#   politicians_test_dict[sent] = list(politicians_test_dict[sent])\n","\n","# for sent in matches_act:\n","#   matches_act[sent] = list(matches_act[sent])\n","# for sent in matches_cric:\n","#   matches_cric[sent] = list(matches_cric[sent])\n","for sent in matches_pol:\n","  matches_pol[sent] = list(matches_pol[sent])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wziGxMplK-Sh"},"outputs":[],"source":["# # saving the input test files\n","# with open(\"/content//drive//MyDrive//Transformer Output//actors_test.json\", \"w\") as outfile: \n","#     json.dump(actors_test_dict,outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/cricketers_test.json\", \"w\") as outfile: \n","#     json.dump(cricketers_test_dict, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/politicians_test.json\", \"w\") as outfile: \n","#     json.dump(politicians_test_dict, outfile)\n","\n","# #saving the output files\n","# with open(\"/content/drive/MyDrive/Transformer Output/actors_matches.json\", \"w\") as outfile: \n","#     json.dump(matches_act, outfile)\n","# with open(\"/content/drive/MyDrive/Transformer Output/cricketers_matches.json\", \"w\") as outfile: \n","#     json.dump(matches_cric, outfile)\n","with open(\"/content/drive/MyDrive/Transformer Output/politicians_matches_ner.json\", \"w\") as outfile: \n","    json.dump(matches_pol, outfile)"]},{"cell_type":"markdown","metadata":{"id":"hjE6hl5KGYQb"},"source":["#### We do find quite a few of the matching triples to be relevant to the sentence. But, there are a few irrelevant matches as well.\n","Upon analysis, we think the word overlap is working better than the vector similarity approach. A possible reason can be that when we simply average out the words in a sentence, and when we average out the words in the triples and then find the similarity between these two averages, some semantic information is lost. So, triples that should have been irrelevant are also found as similar. As the word overlap method is a strictly string overlap, the relevance is much stronger."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Alignment Experiments on NEW_data.ipynb","provenance":[{"file_id":"1d6t3oqMu1gJ3f1dR27YqUA_Dlf_XuzzT","timestamp":1626243987636}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}